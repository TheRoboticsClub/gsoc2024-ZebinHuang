
@article{paniegoBehaviorMetricsOpensource2024,
  title      = {Behavior metrics: An open-source assessment tool for autonomous driving tasks},
  volume     = {26},
  issn       = {2352-7110},
  shorttitle = {Behavior metrics},
  url        = {https://www.softxjournal.com/article/S2352-7110(24)00073-6/fulltext},
  doi        = {10.1016/j.softx.2024.101702},
  language   = {English},
  urldate    = {2024-05-24},
  journal    = {SoftwareX},
  author     = {Paniego, Sergio and Calvo-Palomino, Roberto and Cañas, JoséMaría},
  month      = may,
  year       = {2024},
  note       = {Publisher: Elsevier},
  keywords   = {Imitation learning, Autonomous driving, Evaluation tool},
  file       = {Full Text PDF:/Users/zebin/SynologyDrive/Zotero/storage/U5JHBPF4/Paniego et al. - 2024 - Behavior metrics An open-source assessment tool f.pdf:application/pdf}
}

@article{paniegoModelOptimizationDeep2024,
  title    = {Model {Optimization} in {Deep} {Learning} {Based} {Robot} {Control} for {Autonomous} {Driving}},
  volume   = {9},
  issn     = {2377-3766},
  url      = {https://ieeexplore.ieee.org/document/10328061},
  doi      = {10.1109/LRA.2023.3336244},
  abstract = {Deep learning (DL) has been successfully used in robotics for perception tasks and end-to-end robot control. In the context of autonomous driving, this work explores and compares a variety of alternatives for model optimization to solve the visual lane-follow application in urban scenarios with an imitation learning approach. The optimization techniques include quantization, pruning, fine-tuning (retraining), and clustering, covering all the options available at the most common DL frameworks. TensorRT optimization for specific cutting-edge hardware devices has been also explored. For the comparison, offline metrics such as mean squared error and inference time are used. In addition, the optimized models have been evaluated in an online fashion using the autonomous driving state-of-the-art simulator CARLA and an assessment tool called Behavior Metrics, which provides holistic quantitative fine-grain data about robot performance. Typically the performance of robot applications depends both on the quality of the control decisions and also on their frequency. The studied optimized models significantly increase inference frequency without losing decision quality. The impact of each optimization alone has also been measured. This speed-up allows us to successfully run DL robot-control applications even in limited computing hardware. All the work presented here is open-source, including models, weights, assessment tool, and dataset, for easy replication and extension.},
  number   = {1},
  urldate  = {2024-05-24},
  journal  = {IEEE Robotics and Automation Letters},
  author   = {Paniego, Sergio and Paliwal, Nikhil and Cañas, JoséMaría},
  month    = jan,
  year     = {2024},
  note     = {Conference Name: IEEE Robotics and Automation Letters},
  keywords = {Data models, Robots, Robot sensing systems, Measurement, Optimization, Autonomous vehicles, Deep learning, Imitation learning, machine learning for robot control, sensorimotor learning},
  pages    = {715--722},
  file     = {IEEE Xplore Full Text PDF:/Users/zebin/SynologyDrive/Zotero/storage/WP44AGZI/Paniego et al. - 2024 - Model Optimization in Deep Learning Based Robot Co.pdf:application/pdf}
}

@article{paniegoAutonomousDrivingTraffic2024,
  title    = {Autonomous driving in traffic with end-to-end vision-based deep learning},
  volume   = {594},
  issn     = {09252312},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0925231224006453},
  doi      = {10.1016/j.neucom.2024.127874},
  abstract = {This paper presents a shallow end-to-end vision-based deep learning approach for autonomous vehicle driving in traffic scenarios. The primary objectives include lane keeping and maintaining a safe distance from preceding vehicles. This study leverages an imitation learning approach, creating a supervised dataset for robot control from expert agent demonstrations using the state-of-the-art Carla simulator in different traffic conditions. This dataset encompasses three different versions complementary to each other and we have made it publicly available along with the rest of the materials. The PilotNet neural model is utilized in two variants: the first one with complementary outputs for brake and throttle control commands along with dropout; the second one incorporates these improvements and adds the vehicle speed. Both models have been trained with the aforementioned dataset. The experimental results demonstrate that the models, despite their simplicity and shallow architecture, including only small-scale changes, successfully drive in traffic conditions without sacrificing performance in free-road environments, broadening their area of application widely. Additionally, the second model adeptly maintains a safe distance from leading cars and exhibits satisfactory generalization capabilities to diverse vehicle types. A new evaluation metric to measure the distance to the front vehicle has been created and added to Behavior Metrics; an open-source autonomous driving assessment tool built on CARLA that performs experimental validations of autonomous driving solutions.},
  language = {en},
  urldate  = {2024-05-24},
  journal  = {Neurocomputing},
  author   = {Paniego, Sergio and Shinohara, Enrique and Cañas, JoséMaría},
  month    = aug,
  year     = {2024},
  pages    = {127874},
  file     = {Paniego et al. - 2024 - Autonomous driving in traffic with end-to-end visi.pdf:/Users/zebin/SynologyDrive/Zotero/storage/CR8GRFDF/Paniego et al. - 2024 - Autonomous driving in traffic with end-to-end visi.pdf:application/pdf}
}

@misc{WayveaiDrivingwithLLMs2024,
  title     = {wayveai/{Driving}-with-{LLMs}},
  copyright = {Apache-2.0},
  url       = {https://github.com/wayveai/Driving-with-LLMs},
  abstract  = {PyTorch implementation for the paper "Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving"},
  urldate   = {2024-05-26},
  publisher = {Wayve},
  month     = may,
  year      = {2024},
  note      = {original-date: 2023-05-23T19:48:47Z},
  keywords  = {/unread}
}

@misc{niVincentNi0107BadVLMDriver2024,
  title    = {{VincentNi0107}/{BadVLMDriver}},
  url      = {https://github.com/VincentNi0107/BadVLMDriver},
  urldate  = {2024-05-26},
  author   = {Ni, Zhenyang},
  month    = may,
  year     = {2024},
  note     = {original-date: 2024-03-04T06:36:38Z},
  keywords = {/unread}
}

@misc{huangGPT4VTakesWheel2024a,
  title      = {GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction},
  shorttitle = {{GPT}-{4V} {Takes} the {Wheel}},
  url        = {http://arxiv.org/abs/2311.14786},
  abstract   = {Predicting pedestrian behavior is the key to ensure safety and reliability of autonomous vehicles. While deep learning methods have been promising by learning from annotated video frame sequences, they often fail to fully grasp the dynamic interactions between pedestrians and traffic, crucial for accurate predictions. These models also lack nuanced common sense reasoning. Moreover, the manual annotation of datasets for these models is expensive and challenging to adapt to new situations. The advent of Vision Language Models (VLMs) introduces promising alternatives to these issues, thanks to their advanced visual and causal reasoning skills. To our knowledge, this research is the first to conduct both quantitative and qualitative evaluations of VLMs in the context of pedestrian behavior prediction for autonomous driving. We evaluate GPT4V(ision) on publicly available pedestrian datasets: JAAD and WiDEVIEW. Our quantitative analysis focuses on GPT-4V’s ability to predict pedestrian behavior in current and future frames. The model achieves a 57\% accuracy in a zero-shot manner, which, while impressive, is still behind the state-of-the-art domainspecific models (70\%) in predicting pedestrian crossing actions. Qualitatively, GPT-4V shows an impressive ability to process and interpret complex traffic scenarios, differentiate between various pedestrian behaviors, and detect and analyze groups. However, it faces challenges, such as difficulty in detecting smaller pedestrians and assessing the relative motion between pedestrians and the ego vehicle.},
  language   = {en},
  urldate    = {2024-05-26},
  publisher  = {arXiv},
  author     = {Huang, Jia and Jiang, Peng and Gautam, Alvika and Saripalli, Srikanth},
  month      = jan,
  year       = {2024},
  note       = {arXiv:2311.14786 [cs]},
  keywords   = {Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
  file       = {Huang et al. - 2024 - GPT-4V Takes the Wheel Promises and Challenges fo.pdf:/Users/zebin/SynologyDrive/Zotero/storage/XAWVFQPU/Huang et al. - 2024 - GPT-4V Takes the Wheel Promises and Challenges fo.pdf:application/pdf}
}

@inproceedings{arenasHowPromptYour2023,
  title      = {How to {Prompt} {Your} {Robot}: {A} {PromptBook} for {Manipulation} {Skills} with {Code} as {Policies}},
  shorttitle = {How to {Prompt} {Your} {Robot}},
  url        = {https://openreview.net/forum?id=T8AiZj1QdN},
  abstract   = {Large Language Models (LLMs) have demonstrated the ability to perform semantic reasoning, planning and code writing for robotics tasks. However, most methods rely on pre-existing primitives (i.e. pick, open drawer), which heavily limits their scalability to new scenarios. Additionally, existing approaches like Code as Policies (CaP) rely on examples of robot code in the prompt to write code for new tasks, and assume that LLMs can infer task information, constraints, and API usage from examples alone. But examples can be costly, and too few or too many can bias the LLM in the wrong direction. Recent research has demonstrated prompting LLMs with APIs and documentation enables code writing for successful zero-shot tool use. However, documenting robotics tasks and naively providing full robot APIs presents a challenge to context-length limits in LLMs. In this work, we introduce PromptBook, a recipe that combines LLM prompting paradigms - examples, APIs, documentation and chain of thought, to generate code for planning a sorting task with higher success rate than previous works. We further demonstrate PromptBook enables LLMs to write code for new low-level manipulation primitives in a zero-shot manner: from picking diverse objects, opening/closing drawers, to whisking, and waving hello. We evaluate the new skills on a mobile manipulator with 83{\textbackslash}\% success rate at picking, 50-71\% at opening drawers and 100{\textbackslash}\% at closing them. Notably, the LLM is able to infer gripper orientation for grasping a drawer handle (z-axis aligned) vs. a top-down grasp (x-axis aligned). Finally, we provide guidelines to leverage human feedback and LLMs to write PromptBook prompts.},
  language   = {en},
  urldate    = {2024-05-26},
  author     = {Arenas, Montserrat Gonzalez and Xiao, Ted and Singh, Sumeet and Jain, Vidhi and Ren, Allen Z. and Vuong, Quan and Varley, Jake and Herzog, Alexander and Leal, Isabel and Kirmani, Sean and Sadigh, Dorsa and Sindhwani, Vikas and Rao, Kanishka and Liang, Jacky and Zeng, Andy},
  month      = oct,
  year       = {2023},
  file       = {Full Text PDF:/Users/zebin/SynologyDrive/Zotero/storage/DFHZZQPY/Arenas et al. - 2023 - How to Prompt Your Robot A PromptBook for Manipul.pdf:application/pdf}
}

@misc{shaoLMDriveClosedLoopEndtoEnd2023a,
  title      = {LMDrive: Closed-Loop End-to-End Driving with Large Language Models},
  shorttitle = {{LMDrive}},
  url        = {http://arxiv.org/abs/2312.07488},
  doi        = {10.48550/arXiv.2312.07488},
  abstract   = {Despite significant recent progress in the field of autonomous driving, modern methods still struggle and can incur serious accidents when encountering long-tail unforeseen events and challenging urban scenarios. On the one hand, large language models (LLM) have shown impressive reasoning capabilities that approach "Artificial General Intelligence". On the other hand, previous autonomous driving methods tend to rely on limited-format inputs (e.g. sensor data and navigation waypoints), restricting the vehicle's ability to understand language information and interact with humans. To this end, this paper introduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous driving framework. LMDrive uniquely processes and integrates multi-modal sensor data with natural language instructions, enabling interaction with humans and navigation software in realistic instructional settings. To facilitate further research in language-based closed-loop autonomous driving, we also publicly release the corresponding dataset which includes approximately 64K instruction-following data clips, and the LangAuto benchmark that tests the system's ability to handle complex instructions and challenging driving scenarios. Extensive closed-loop experiments are conducted to demonstrate LMDrive's effectiveness. To the best of our knowledge, we're the very first work to leverage LLMs for closed-loop end-to-end autonomous driving. Codes, models, and datasets can be found at https://github.com/opendilab/LMDrive},
  urldate    = {2024-05-26},
  publisher  = {arXiv},
  author     = {Shao, Hao and Hu, Yuxuan and Wang, Letian and Waslander, Steven L. and Liu, Yu and Li, Hongsheng},
  month      = dec,
  year       = {2023},
  note       = {arXiv:2312.07488 [cs]},
  keywords   = {Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: project page: https://hao-shao.com/projects/lmdrive.html},
  file       = {arXiv Fulltext PDF:/Users/zebin/SynologyDrive/Zotero/storage/JI6U4T5B/Shao et al. - 2023 - LMDrive Closed-Loop End-to-End Driving with Large.pdf:application/pdf;arXiv.org Snapshot:/Users/zebin/SynologyDrive/Zotero/storage/NEZNN9GJ/2312.html:text/html}
}

@misc{chenDrivingLLMsFusing2023a,
  title      = {Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving},
  shorttitle = {Driving with {LLMs}},
  url        = {http://arxiv.org/abs/2310.01957},
  doi        = {10.48550/arXiv.2310.01957},
  abstract   = {Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available for further exploration.},
  urldate    = {2024-05-26},
  publisher  = {arXiv},
  author     = {Chen, Long and Sinavski, Oleg and Hünermann, Jan and Karnsund, Alice and Willmott, Andrew James and Birch, Danny and Maund, Daniel and Shotton, Jamie},
  month      = oct,
  year       = {2023},
  note       = {arXiv:2310.01957 [cs]},
  keywords   = {Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
  file       = {arXiv Fulltext PDF:/Users/zebin/SynologyDrive/Zotero/storage/VRYSQX2I/Chen et al. - 2023 - Driving with LLMs Fusing Object-Level Vector Moda.pdf:application/pdf;arXiv.org Snapshot:/Users/zebin/SynologyDrive/Zotero/storage/7S3UVAPU/2310.html:text/html}
}

@misc{kwonLanguageModelsZeroShot2023,
  title     = {Language Models as Zero-Shot Trajectory Generators},
  url       = {http://arxiv.org/abs/2310.11604},
  doi       = {10.48550/arXiv.2310.11604},
  abstract  = {Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding of low-level robot control sufficient for a range of common tasks, and that they can additionally detect failures and then re-plan trajectories accordingly. Videos, code, and prompts are available at: https://www.robot-learning.uk/language-models-trajectory-generators.},
  urldate   = {2024-05-27},
  publisher = {arXiv},
  author    = {Kwon, Teyun and Di Palo, Norman and Johns, Edward},
  month     = oct,
  year      = {2023},
  note      = {arXiv:2310.11604 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Computation and Language, /unread},
  annote    = {Comment: 19 pages, 21 figures},
  file      = {arXiv Fulltext PDF:/Users/zebin/SynologyDrive/Zotero/storage/KLP2LTBN/Kwon et al. - 2023 - Language Models as Zero-Shot Trajectory Generators.pdf:application/pdf;arXiv.org Snapshot:/Users/zebin/SynologyDrive/Zotero/storage/8R7LBXMA/2310.html:text/html}
}
