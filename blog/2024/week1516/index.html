<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Coding week15&amp;16 9/09-9/22 | Zebin Huang | JdeRobot x GSoC2024 </title> <meta name="author" content="Zebin Huang"> <meta name="description" content="Zebin Huang | JdeRobot x GSoC2024 "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/gsoc2024-ZebinHuang/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/gsoc2024-ZebinHuang/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/gsoc2024-ZebinHuang/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/gsoc2024-ZebinHuang/assets/img/logo.png?a3cac8c2613225704682d343b37ffd84"> <link rel="stylesheet" href="/gsoc2024-ZebinHuang/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week1516/"> <script src="/gsoc2024-ZebinHuang/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/gsoc2024-ZebinHuang/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/gsoc2024-ZebinHuang/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/gsoc2024-ZebinHuang/assets/js/distillpub/template.v2.js"></script> <script src="/gsoc2024-ZebinHuang/assets/js/distillpub/transforms.v2.js"></script> <script src="/gsoc2024-ZebinHuang/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Coding week15&16 9/09-9/22",
            "description": "",
            "published": "September 22, 2024",
            "authors": [
              
              {
                "author": "Zebin Huang",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Edinburgh Centre for Robotics",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/gsoc2024-ZebinHuang/"> Zebin Huang | JdeRobot x GSoC2024 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/gsoc2024-ZebinHuang/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/gsoc2024-ZebinHuang/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/gsoc2024-ZebinHuang/repositories/">repositories </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Coding week15&amp;16 9/09-9/22</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <p>This week, we focused on exploring new ideas for enhancing autonomous vehicle capabilities with LLMs language instructions. Below is a summary of the primary ideas we considered, aiming to address current limitations and expand on innovative interaction methods between passengers and self-driving systems.</p> <p>This week, we focused on exploring new ideas for enhancing autonomous vehicle capabilities with LLMs language instructions.</p> <ul> <li> <p>Idea Selection and Planning: Sharing the complete list of ideas from this meeting and subsequent discussions will allow us to identify and finalize one or two core directions.</p> </li> <li> <p>Feasibility Research: We will also conduct feasibility studies on the selected ideas to explore technical challenges and identifying necessary tools, frameworks, and methodologies. This research will be crucial in ensuring that the chosen solutions are practically implementable for future development.</p> </li> </ul> <p>Below is a summary of the primary ideas explored.</p> <h2 id="driving-with-natural-language">Driving with Natural Language</h2> <h3 id="talk2car-taking-control-of-your-self-driving-car-">Talk2Car: Taking Control of Your Self-Driving Car <d-cite key="deruyttere_2019_Talk2CarTakingControl"></d-cite> </h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week1516/talk2car-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week1516/talk2car-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week1516/talk2car-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/gsoc2024-ZebinHuang/assets/img/week1516/talk2car.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We began by exploring potential limitations from the early stages of the work.</p> <ul> <li> <p><strong>Motivation</strong>: Autonomous vehicles may hesitate in some complex traffic scenarios, especially when roads are congested or special situations arise. If passengers can provide suggestions or instructions through natural language commands, such as asking the vehicle to stop or wait, it can help the vehicle make quicker decisions.</p> </li> <li> <p><strong>Limited to Object Reference Tasks</strong>: The core issue of the article is to map the passenger’s natural language commands to specific objects in the visual scene. This “object reference” task mainly aims to enable autonomous vehicles to identify specific objects mentioned in passenger instructions, without involving direct vehicle control or executing route planning. Therefore, the article focuses solely on “recognizing reference objects”.</p> </li> <li> <p><strong>Not Involve Real Control</strong>: Although the article discusses the motivation for allowing passengers to interact with autonomous vehicles through natural language, the current research is limited to recognizing objects indicated by passengers through natural language <strong>recognition</strong> and not actually executing these instructions (e.g., stopping, turning, etc.).</p> </li> </ul> <h3 id="conditional-driving-from-natural-language-instructions-">Conditional Driving from Natural Language Instructions <d-cite key="roh20a"></d-cite> </h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week1516/conditional_driving-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week1516/conditional_driving-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week1516/conditional_driving-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/gsoc2024-ZebinHuang/assets/img/week1516/conditional_driving.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Previous work primarily focus on path planning and navigation systems. However, as human-machine interaction evolves, we realize that human expectations for autonomous driving systems extend beyond simple navigation commands to a more “coaching” role in interactions. In such interactions, autonomous driving systems need not only basic path planning capabilities but also the ability to understand complex, multi-turn instructions, demonstrate keen environmental insights, and effectively handle edge or hazardous scenarios. This motivation from this paper has driven the proposal and exploration of the following research directions.</p> <h3 id="talk-to-the-vehicle-language-conditioned-autonomous-navigation-of-self-driving-cars-">Talk to the Vehicle: Language-Conditioned Autonomous Navigation of Self-Driving Cars <d-cite key="sriram_2019_TalkVehicleLanguageb"></d-cite> </h3> <p>The motivation for the article is based on the idea that in many complex road scenarios, traditional autonomous driving technology relies on pre-generated detailed maps and precise positioning systems. When these conditions are not met, such as when maps are inaccurate or GPS delays occur, vehicles may fail to navigate correctly.</p> <p>Human navigation relies not on precise map data but on the <strong>semantic understanding of language and the current environment.</strong> By adopting this method, autonomous vehicles can navigate through semantic understanding and natural language instructions without relying on detailed offline maps.</p> <p>By introducing natural language instructions, vehicles can complete navigation tasks without accurate maps or positioning information. This reduces the dependence on external maps and GPS accuracy. This would make autonomous driving technology more robust and adaptable, especially in uncontrollable outdoor environments.</p> <p>Therefore, the research motivation is to explore how to enhance the navigation capabilities of autonomous vehicles through natural language instructions. This approach can not only improve the efficiency of autonomous driving but also allow autonomous vehicles to integrate more naturally into human driving behaviors, especially in the face of GPS positioning errors or the absence of pre-mapped maps.</p> <p>The primary inspiration for this paper is that, <strong>in real life, human drivers can easily complete navigation tasks with simple language instructions (such as “turn right at the traffic light, then turn left at the second intersection”) even without precise maps.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week1516/framework_talk2vehicle-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week1516/framework_talk2vehicle-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week1516/framework_talk2vehicle-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/gsoc2024-ZebinHuang/assets/img/week1516/framework_talk2vehicle.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The framework proposed in the article has three core modules:</p> <ul> <li>NLE: Transforms natural language instructions into high-level machine-readable encodings.</li> <li>Waypoint Generation Network (WGN): Combines local semantic structure with language encoding to predict local waypoints.</li> <li>Generates obstacle-avoidance trajectories based on predicted waypoints, executed by the low-level controller.</li> <li>By combining natural language with visual and semantic maps, vehicles can generate waypoints adapted to the current environment, and this method avoids dependence on detailed maps and precise positioning. Each time local waypoints are generated, WGN considers the language instructions and the vehicle’s current environmental information.</li> </ul> <h2 id="long-turn-interaction">Long-Turn Interaction</h2> <h3 id="drivlme-enhancing-llm-based-autonomous-driving-agents-with-embodied-and-social-experiences-">DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences <d-cite key="huang__DriVLMeEnhancingLLMbased"></d-cite> </h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week1516/talk2vehicle-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week1516/talk2vehicle-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week1516/talk2vehicle-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/gsoc2024-ZebinHuang/assets/img/week1516/talk2vehicle.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The motivation of the article is to address the limitations of existing autonomous driving technologies in actual complex driving scenarios, especially the problems of <strong>long-duration navigation tasks</strong> and <strong>free dialogue interactions</strong>. Although current foundational models (FMs) have shown the potential to handle short-term tasks, they still face challenges when dealing with environmental dynamics, task changes, or long-term human-vehicle interactions:</p> <p>Existing autonomous driving systems are proficient at executing simple, short-term tasks like turning or overtaking but fall short when it comes to understanding broader, goal-oriented tasks that involve route planning and map knowledge. Additionally, these systems struggle to handle unexpected situations that arise from sensor limitations, environmental changes, or shifts in task requirements. Traditional systems are also limited in managing natural language dialogue, making it difficult for them to engage in complex, multi-turn interactions with passengers, especially in dynamic and evolving environments where continuous context understanding and appropriate responses are essential.</p> <h3 id="dorothie-spoken-dialogue-for-handling-unexpected-situations-in-interactive-autonomous-driving-agents-">DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents <d-cite key="ma_2022_DOROTHIESpokenDialogue"></d-cite> </h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week1516/multi_interaction-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week1516/multi_interaction-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week1516/multi_interaction-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/gsoc2024-ZebinHuang/assets/img/week1516/multi_interaction.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Motivation</strong>: User interaction with autonomous driving systems is often multi-turn, involving complex, long-duration instruction processing. To achieve this, the system must have the capability to handle long-term instructions, such as mapless navigation (Mapless Navigation) and following complex instructions (Multi-turn Interactions, Long Horizon Instructions). Traditional navigation systems only need to provide a path and execute it, but the new interaction mode requires the system to flexibly adapt to new instructions, even dynamically adjusting paths and goals during the task process.</p> <h3 id="potential-research-directions">Potential Research Directions</h3> <ol> <li> <p>Handling Ambiguity in Language Instructions: Language often contains vague or uncertain expressions, like “go forward a bit, then turn right.” Without adequate context-understanding, such vague instructions may lead to navigation errors.</p> <ul> <li>Potential research direction: Employing advanced language models (e.g., GPT series) that are better equipped to interpret ambiguous language, handle diverse expressions, and reason based on context.</li> </ul> </li> <li> <p>Semantic Understanding of Environment:</p> <p>The current system uses a waypoint generation network that combines semantic information and language encoding to produce local waypoints. However, this integration is still “static.”</p> <ul> <li>Challenges: <ul> <li>The system assumes that combining language encoding with the current semantic map suffices, but real-world scenarios often involve mismatches between environmental perception and language instructions. For instance, there could be multiple “left turns,” making it unclear which one to follow.</li> <li>While LLMs can handle complex multimodal data, the current system lacks the deeper interaction and reasoning needed to optimally use environmental and language information together.</li> </ul> </li> </ul> </li> <li> <p>Lack of Long-Term Understanding:</p> <p>Although the system’s local planning approach offers flexibility, it lacks a global perspective. Only planning local waypoints can lead to that local planning may not always find the most efficient path, potentially leading to detours or longer routes.</p> </li> <li> <p>Inadequate Exception Handling Capabilities:</p> <p>The system’s reliance on real-time sensor input and waypoint updates can make it vulnerable to unexpected situations if sensor data fails or feedback is inaccurate.</p> <ul> <li>Challenges: <ul> <li>The current system might fail to respond effectively to emergencies, such as unexpected obstacles or environmental changes.</li> <li>Although LLMs provide robust reasoning and language processing, integrating these models with dynamic environmental feedback remains limited.</li> </ul> </li> <li>Potential research direction: Investigating RL or feedback mechanisms based on historical data to help the system adjust autonomously during emergencies and dynamically optimize its path. Additionally, integrating LLMs with sensor feedback could enhance response to complex conditions.</li> </ul> </li> </ol> <h3 id="advanced-frameworks">Advanced Frameworks</h3> <ol> <li> <p>World Model Based on Real Data Research increasingly focuses on using real data to train systems for better environmental cognition, enabling systems not only to process immediate perceptual data but also to predict and anticipate complex traffic situations. This model aids in handling variable traffic scenarios, especially in dynamically changing or uncertain environments <d-cite key="guan_2024_WorldModelsAutonomous"></d-cite>.</p> </li> <li> <p>Language Generation and Embodied Experiences Language generation should go beyond preset corpora by incorporating embodied experiences. Autonomous driving systems can benefit from extracting and interpreting information from the environment and translating it into natural language feedback. In complex traffic situations, the system could assess road conditions in real-time and communicate this to users.</p> </li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/gsoc2024-ZebinHuang/assets/bibliography/week1516.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Zebin Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/gsoc2024-ZebinHuang/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>