<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/feed.xml" rel="self" type="application/atom+xml"/><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-24T10:14:53+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/feed.xml</id><title type="html">Zebin Huang | JdeRobot x GSoC2024</title><subtitle>Zebin Huang | JdeRobot x GSoC2024 </subtitle><entry><title type="html">Coding week2 6/03-6/09</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/coding-week2-6-03-6-09/" rel="alternate" type="text/html" title="Coding week2 6/03-6/09"/><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week2</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/coding-week2-6-03-6-09/"><![CDATA[<h3 id="weekly-meeting">Weekly Meeting</h3> <p>In this week’s meeting, we reviewed our project’s current status. We analyzed the high similarity observed in the outputs generated by GPT and discussed the issues of data distribution, which led to redundancy. To address these concerns, we brainstormed strategies to enhance the diversity and balance of our dataset. Additionally, we revisited four key commands from our previous project and explored how integrating more commands could boost both functionality and efficiency. We also deliberated on setting appropriate metrics for the BERT model by segmenting the dataset into training and testing sets.</p> <p>More details can be found here: <a href="https://docs.google.com/document/d/1b2ZEU5Gt8gP2ae_YzNSJSd7RukUrsG_aDJFLnbvoQiM/edit">Google Doc</a></p> <h3 id="to-do-list">To-Do List</h3> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Generate scalable datasets that are intertwined with actions and commands with LLMs.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Perform quality analyses on the generated datasets.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Train the Bert model to classify instructions and obtain a model.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Move training and test notebooks in gsoc23 to a separate script. <a href="https://github.com/TheRoboticsClub/gsoc2023-Meiqi_Zhao/issues/5">issue</a></li> </ul> <h3 id="progress">Progress</h3> <p>This week, I developed a module designed to generate user-driving instructions using GPT. The code focuses on creating prompts based on predefined templates and user inputs, with an emphasis on improving the logic for prompt creation to enhance clarity and engagement. Additionally, the module includes features for validating user inputs and adjusting the output format accordingly. This advancement is beneficial for providing scalable instruction datasets in our project.</p> <p>The module also implements an action generation component, which formulates specific actions based on the instructions generated by GPT, such as “turn left,” “turn right,” “take exit,” “go straight,” “accelerate,” and “slow down.” Furthermore, I developed analytical tools within the prompt_analysis.py script to evaluate the effectiveness of the generated instructions, incorporating metrics to assess their relevance. Lastly, I implemented training using the BERT model to further enhance the module’s performance.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train.py
1000 1000
Labels length: 1000
Input IDs length: 1000
Attention Mask length: 1000
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: <span class="o">[</span><span class="s1">'classifier.bias'</span>, <span class="s1">'classifier.weight'</span><span class="o">]</span>
You should probably TRAIN this model on a down-stream task to be able to use it <span class="k">for </span>predictions and inference.
<span class="o">{</span><span class="s1">'loss'</span>: 1.697, <span class="s1">'grad_norm'</span>: 19.306570053100586, <span class="s1">'learning_rate'</span>: 1.0000000000000002e-06, <span class="s1">'epoch'</span>: 0.09<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 1.617, <span class="s1">'grad_norm'</span>: 13.708064079284668, <span class="s1">'learning_rate'</span>: 2.0000000000000003e-06, <span class="s1">'epoch'</span>: 0.18<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 1.4665, <span class="s1">'grad_norm'</span>: 9.595329284667969, <span class="s1">'learning_rate'</span>: 3e-06, <span class="s1">'epoch'</span>: 0.27<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 1.2224, <span class="s1">'grad_norm'</span>: 12.889700889587402, <span class="s1">'learning_rate'</span>: 4.000000000000001e-06, <span class="s1">'epoch'</span>: 0.35<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 1.0262, <span class="s1">'grad_norm'</span>: 10.967733383178711, <span class="s1">'learning_rate'</span>: 5e-06, <span class="s1">'epoch'</span>: 0.44<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.7739, <span class="s1">'grad_norm'</span>: 13.827258110046387, <span class="s1">'learning_rate'</span>: 6e-06, <span class="s1">'epoch'</span>: 0.53<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.7852, <span class="s1">'grad_norm'</span>: 5.5881853103637695, <span class="s1">'learning_rate'</span>: 7.000000000000001e-06, <span class="s1">'epoch'</span>: 0.62<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.6525, <span class="s1">'grad_norm'</span>: 8.820500373840332, <span class="s1">'learning_rate'</span>: 8.000000000000001e-06, <span class="s1">'epoch'</span>: 0.71<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.3866, <span class="s1">'grad_norm'</span>: 4.9470295906066895, <span class="s1">'learning_rate'</span>: 9e-06, <span class="s1">'epoch'</span>: 0.8<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.3203, <span class="s1">'grad_norm'</span>: 3.308020830154419, <span class="s1">'learning_rate'</span>: 1e-05, <span class="s1">'epoch'</span>: 0.88<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.3612, <span class="s1">'grad_norm'</span>: 9.319038391113281, <span class="s1">'learning_rate'</span>: 1.1000000000000001e-05, <span class="s1">'epoch'</span>: 0.97<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.3051, <span class="s1">'grad_norm'</span>: 8.626148223876953, <span class="s1">'learning_rate'</span>: 1.2e-05, <span class="s1">'epoch'</span>: 1.06<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.3143, <span class="s1">'grad_norm'</span>: 4.779806613922119, <span class="s1">'learning_rate'</span>: 1.3000000000000001e-05, <span class="s1">'epoch'</span>: 1.15<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.2742, <span class="s1">'grad_norm'</span>: 8.452868461608887, <span class="s1">'learning_rate'</span>: 1.4000000000000001e-05, <span class="s1">'epoch'</span>: 1.24<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.3953, <span class="s1">'grad_norm'</span>: 3.7451024055480957, <span class="s1">'learning_rate'</span>: 1.5e-05, <span class="s1">'epoch'</span>: 1.33<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.2989, <span class="s1">'grad_norm'</span>: 0.8715028762817383, <span class="s1">'learning_rate'</span>: 1.6000000000000003e-05, <span class="s1">'epoch'</span>: 1.42<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.3149, <span class="s1">'grad_norm'</span>: 9.316072463989258, <span class="s1">'learning_rate'</span>: 1.7000000000000003e-05, <span class="s1">'epoch'</span>: 1.5<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.1399, <span class="s1">'grad_norm'</span>: 8.054197311401367, <span class="s1">'learning_rate'</span>: 1.8e-05, <span class="s1">'epoch'</span>: 1.59<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.246, <span class="s1">'grad_norm'</span>: 7.560857772827148, <span class="s1">'learning_rate'</span>: 1.9e-05, <span class="s1">'epoch'</span>: 1.68<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.151, <span class="s1">'grad_norm'</span>: 0.3591634929180145, <span class="s1">'learning_rate'</span>: 2e-05, <span class="s1">'epoch'</span>: 1.77<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.1007, <span class="s1">'grad_norm'</span>: 7.88149881362915, <span class="s1">'learning_rate'</span>: 2.1e-05, <span class="s1">'epoch'</span>: 1.86<span class="o">}</span>
 63%|█████████████████████████████████████████████████████████████████████████████████▏                                              | 215/33 64%|████████████████████████████████████████████████████████████████▉                                     | 216/339 <span class="o">[</span>00:55&lt;00:31,  3.93it/s]<span class="o">{</span><span class="s1">'loss'</span>: 0.1223, <span class="s1">'grad_norm'</span>: 1.4635449647903442, <span class="s1">'learning_rate'</span>: 2.2000000000000003e-05, <span class="s1">'epoch'</span>: 1.95<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.1039, <span class="s1">'grad_norm'</span>: 17.848655700683594, <span class="s1">'learning_rate'</span>: 2.3000000000000003e-05, <span class="s1">'epoch'</span>: 2.04<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.2297, <span class="s1">'grad_norm'</span>: 0.22711549699306488, <span class="s1">'learning_rate'</span>: 2.4e-05, <span class="s1">'epoch'</span>: 2.12<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.2394, <span class="s1">'grad_norm'</span>: 0.3263954222202301, <span class="s1">'learning_rate'</span>: 2.5e-05, <span class="s1">'epoch'</span>: 2.21<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.2566, <span class="s1">'grad_norm'</span>: 0.4583888351917267, <span class="s1">'learning_rate'</span>: 2.6000000000000002e-05, <span class="s1">'epoch'</span>: 2.3<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.1733, <span class="s1">'grad_norm'</span>: 0.12949731945991516, <span class="s1">'learning_rate'</span>: 2.7000000000000002e-05, <span class="s1">'epoch'</span>: 2.39<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.1403, <span class="s1">'grad_norm'</span>: 0.12070054560899734, <span class="s1">'learning_rate'</span>: 2.8000000000000003e-05, <span class="s1">'epoch'</span>: 2.48<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.2578, <span class="s1">'grad_norm'</span>: 0.15354938805103302, <span class="s1">'learning_rate'</span>: 2.9e-05, <span class="s1">'epoch'</span>: 2.57<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.1055, <span class="s1">'grad_norm'</span>: 0.30441755056381226, <span class="s1">'learning_rate'</span>: 3e-05, <span class="s1">'epoch'</span>: 2.65<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.2079, <span class="s1">'grad_norm'</span>: 16.82185935974121, <span class="s1">'learning_rate'</span>: 3.1e-05, <span class="s1">'epoch'</span>: 2.74<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.2025, <span class="s1">'grad_norm'</span>: 4.622957229614258, <span class="s1">'learning_rate'</span>: 3.2000000000000005e-05, <span class="s1">'epoch'</span>: 2.83<span class="o">}</span>
<span class="o">{</span><span class="s1">'loss'</span>: 0.1915, <span class="s1">'grad_norm'</span>: 18.763633728027344, <span class="s1">'learning_rate'</span>: 3.3e-05, <span class="s1">'epoch'</span>: 2.92<span class="o">}</span>
<span class="o">{</span><span class="s1">'train_runtime'</span>: 84.7519, <span class="s1">'train_samples_per_second'</span>: 31.858, <span class="s1">'train_steps_per_second'</span>: 4.0, <span class="s1">'train_loss'</span>: 0.45582248397984687, <span class="s1">'epoch'</span>: 3.0<span class="o">}</span>
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 339/339 <span class="o">[</span>01:24&lt;00:00,  4.00it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 <span class="o">[</span>00:00&lt;00:00, 11.49it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 <span class="o">[</span>00:00&lt;00:00, 11.14it/s]
              precision    recall  f1-score   support

           0       0.99      0.96      0.97        89
           1       0.71      0.91      0.80        11

    accuracy                           0.95       100
   macro avg       0.85      0.93      0.89       100
weighted avg       0.96      0.95      0.95       100
</code></pre></div></div> <h3 id="challenges">Challenges</h3> <p>During the data generation process, I encountered a significant issue with data imbalance. By using GPT to generate 1000 data points, I discovered that the distribution of the data was uneven, as illustrated below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/wordcloud-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/wordcloud-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/wordcloud-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/wordcloud.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/datasets-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/datasets-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/datasets-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/datasets.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/distribution-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/distribution-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/distribution-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>From the database analysis, we observed an increased presence of data duplicates, such as the instruction “Approaching the roundabout.” Consequently, the model’s interpretation of this instruction has predominantly skewed towards “take exit.” This repetition is also evident in the word cloud, where several words appear repeatedly. This issue results in an unbalanced distribution of data, which affects the model’s performance and accuracy in understanding diverse driving instructions.</p> <p>Although the BERT model achieved satisfactory accuracy, the skewed data distribution poses challenges for real-world application. The model’s performance might not generalize well to new, unseen data if it does not reflect a balanced representation of all possible scenarios. This imbalance could lead to biased predictions and reduced effectiveness in practical use cases. Addressing this challenge will be crucial to ensure the robustness and reliability of the instructional generation system. Potential solutions include augmenting the dataset to ensure balance or applying bias correction methods during the model training phase.</p> <h3 id="future-tasks">Future Tasks</h3> <ul> <li>Continue refining the data balancing strategies to further improve the diversity of generated outputs.</li> <li>Complete the integration and testing of additional commands from previous projects.</li> <li>Optimize the model training process and evaluate the effectiveness of the dataset division.</li> </ul>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[Weekly Meeting]]></summary></entry><entry><title type="html">Set up CARLA with Docker</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/carla-docker/" rel="alternate" type="text/html" title="Set up CARLA with Docker"/><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/carla-docker</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/carla-docker/"><![CDATA[<p>This guide will walk you through the process of setting up the CARLA simulator (version 0.9.14) using Docker.</p> <h2 id="prerequisites">Prerequisites</h2> <ul> <li>Docker installed on your system.</li> <li>NVIDIA Docker if you are using NVIDIA GPUs for rendering.</li> </ul> <h2 id="pull-the-carla-docker-image">Pull the CARLA Docker Image</h2> <p>Start by pulling the official CARLA image from Docker Hub:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker pull carlasim/carla:0.9.14
</code></pre></div></div> <h2 id="run-carla-in-a-docker-container">Run CARLA in a Docker Container</h2> <p>To run CARLA with Docker, you can use the following command. This setup forwards necessary ports and configures the environment for GPU usage and offscreen rendering:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>docker run <span class="nt">-p</span> 2000-2002:2000-2002 <span class="nt">--privileged</span> <span class="nt">--gpus</span> all <span class="nt">--net</span><span class="o">=</span>host <span class="nt">-v</span> /tmp/.X11-unix:/tmp/.X11-unix:rw carlasim/carla:0.9.14 /bin/bash ./CarlaUE4.sh <span class="nt">-RenderOffScreen</span>
</code></pre></div></div> <h2 id="installing-carla-and-its-python-api">Installing CARLA and its Python API</h2> <h3 id="installing-carla-on-debian-based-systems">Installing CARLA on Debian-based Systems</h3> <p>Debian packages of CARLA are available for both Ubuntu 18.04 (Bionic Beaver) and Ubuntu 20.04 (Focal Fossa). However, the officially supported version is Ubuntu 18.04. Here’s how to install CARLA using the Debian package repository:</p> <ol> <li> <p><strong>Add the CARLA Repository to Your System:</strong> This step involves adding the GPG key for the CARLA repository to your system and then adding the repository itself.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nb">sudo </span>apt-key adv <span class="nt">--keyserver</span> keyserver.ubuntu.com <span class="nt">--recv-keys</span> 1AF1527DE64CB8D9
 <span class="nb">sudo </span>add-apt-repository <span class="s2">"deb [arch=amd64] http://dist.carla.org/carla </span><span class="si">$(</span>lsb_release <span class="nt">-sc</span><span class="si">)</span><span class="s2"> main"</span>
</code></pre></div> </div> </li> <li> <p><strong>Install CARLA:</strong> Update your package list and install CARLA. The installation directory will be <code class="language-plaintext highlighter-rouge">/opt/carla-simulator/</code>.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nb">sudo </span>apt-get update
 <span class="nb">sudo </span>apt-get <span class="nb">install </span>carla-simulator
 <span class="nb">cd</span> /opt/carla-simulator
</code></pre></div> </div> </li> </ol> <h3 id="manual-installation-from-github">Manual Installation from GitHub</h3> <p>If the Debian server is down, as has been reported in issues like <a href="https://github.com/carla-simulator/carla/issues/7017">CARLA GitHub Issue #7017</a>, you can manually install CARLA:</p> <ol> <li> <p><strong>Install Required System Dependency:</strong> Before downloading CARLA, make sure that all necessary dependencies are installed.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nb">sudo </span>apt-get <span class="nt">-y</span> <span class="nb">install </span>libomp5
</code></pre></div> </div> </li> <li> <p><strong>Download CARLA Release:</strong> Use <code class="language-plaintext highlighter-rouge">wget</code> to download the CARLA release directly from its S3 bucket. Adjust the version number as needed for the specific version you are installing.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># Download CARLA version 0.9.14</span>
 wget https://carla-releases.s3.us-east-005.backblazeb2.com/Linux/CARLA_0.9.14.tar.gz
</code></pre></div> </div> </li> <li> <p><strong>Unpack CARLA:</strong> Unpack the downloaded archive to the desired directory, typically <code class="language-plaintext highlighter-rouge">/opt/carla-simulator/</code>.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nb">tar</span> <span class="nt">-xzvf</span> CARLA_0.9.14.tar.gz <span class="nt">-C</span> /opt/carla-simulator/
</code></pre></div> </div> </li> <li> <p><strong>Install the CARLA Python Module:</strong> Install the CARLA Python module and its dependencies to ensure that you can interact with CARLA using Python scripts.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># Ensure pip is up to date</span>
 pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
 <span class="c"># Install CARLA Python API</span>
 python <span class="nt">-m</span> pip <span class="nb">install </span><span class="nv">carla</span><span class="o">==</span>0.9.14
 <span class="c"># Install additional Python dependencies required for CARLA</span>
 python <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">-r</span> /opt/carla-simulator/PythonAPI/examples/requirements.txt
</code></pre></div> </div> </li> </ol> <h2 id="check-the-server-is-running">Check the Server is Running</h2> <p>To verify the status of the CARLA server, there are several methods you can use to check if it is running properly:</p> <ol> <li> <p><strong>Check Container Processes</strong>: If you have launched the CARLA server in a Docker container, you can check the list of processes by running the <code class="language-plaintext highlighter-rouge">ps</code> command. In the command line of your container, enter the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ps aux | <span class="nb">grep </span>CarlaUE4
</code></pre></div> </div> <p>If the CARLA server is running, you should see a process listed that includes <code class="language-plaintext highlighter-rouge">CarlaUE4.sh</code> or a similar name.</p> </li> <li><strong>Check Log Output</strong>: The CARLA server typically outputs logs. If you start the server’s Docker container without running it in the background, you should be able to see the outputs directly in your terminal.</li> <li> <p><strong>Check Network Listening Ports</strong>: The CARLA server listens on specific ports by default (e.g., 2000-2002). On the host machine, you can use the following command to check if these ports are being listened to:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nb">sudo </span>netstat <span class="nt">-tulpn</span> | <span class="nb">grep </span>LISTEN
</code></pre></div> </div> <p>Look for ports <code class="language-plaintext highlighter-rouge">2000</code>, <code class="language-plaintext highlighter-rouge">2001</code>, and <code class="language-plaintext highlighter-rouge">2002</code> in the output to see if any process is bound to these ports. The output should be similar to:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> tcp        0      0 0.0.0.0:2001            0.0.0.0:<span class="k">*</span>               LISTEN      532651/CarlaUE4-Lin
 tcp        0      0 0.0.0.0:2000            0.0.0.0:<span class="k">*</span>               LISTEN      532651/CarlaUE4-Lin
 tcp        0      0 0.0.0.0:2002            0.0.0.0:<span class="k">*</span>               LISTEN      532651/CarlaUE4-Lin
</code></pre></div> </div> </li> <li> <p><strong>Testing Connection with CARLA Python API</strong>: If you have a Python environment set up with the CARLA Python client library, you can try writing a simple script to attempt a connection to the server:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kn">import</span> <span class="n">carla</span>

 <span class="k">try</span><span class="p">:</span>
     <span class="n">client</span> <span class="o">=</span> <span class="n">carla</span><span class="p">.</span><span class="nc">Client</span><span class="p">(</span><span class="sh">'</span><span class="s">localhost</span><span class="sh">'</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
     <span class="n">client</span><span class="p">.</span><span class="nf">set_timeout</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">CARLA server is running!</span><span class="sh">"</span><span class="p">)</span>
     <span class="c1"># Optionally, get some additional data
</span>     <span class="n">world</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">get_world</span><span class="p">()</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Connected to world: </span><span class="sh">"</span><span class="p">,</span> <span class="n">world</span><span class="p">.</span><span class="nf">get_map</span><span class="p">().</span><span class="n">name</span><span class="p">)</span>
 <span class="k">except</span> <span class="nb">RuntimeError</span><span class="p">:</span>
     <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Cannot connect to CARLA server.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> <p>This script will attempt to connect to the CARLA server running locally on port 2000. If the connection is successful, it will print that the server is running.</p> </li> </ol> <h2 id="troubleshooting-common-errors">Troubleshooting Common Errors</h2> <p>When working with CARLA and Docker, you might encounter errors such as connection timeouts or ALSA audio issues. Below are some common errors.</p> <h3 id="connection-timeout">Connection Timeout</h3> <p>After running the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>nvidia-docker run <span class="nt">-p</span> 2000-2002:2000-2002 <span class="nt">-it</span> <span class="nt">--rm</span>  carlasim/carla:0.9.14 /bin/bash
<span class="nv">SDL_VIDEODRIVER</span><span class="o">=</span>offscreen ./CarlaUE4.sh <span class="nt">-opengl</span>
</code></pre></div></div> <p>Error Message:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Cannot connect to CARLA server: time-out of 10000ms <span class="k">while </span>waiting <span class="k">for </span>the simulator, make sure the simulator is ready and connected to localhost:2000
</code></pre></div></div> <p>Solution:</p> <ul> <li>Ensure that the Docker container is running and the ports are correctly mapped.</li> <li> <p>Use the following command to check if CARLA is actively listening on the expected ports:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nb">sudo </span>netstat <span class="nt">-tulpn</span> | <span class="nb">grep </span>LISTEN
</code></pre></div> </div> </li> </ul> <h3 id="alsa-audio-errors">ALSA Audio Errors</h3> <p>When running CARLA, you might see repeated ALSA errors. These generally indicate missing audio configurations, which are common in Docker containers but don’t affect the simulation:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>carla_0914_env<span class="o">)</span> <span class="o">(</span>base<span class="o">)</span> Procuda% docker run <span class="nt">-e</span> <span class="nv">DISPLAY</span><span class="o">=</span><span class="nv">$DISPLAY</span> <span class="nt">-it</span> <span class="nt">--net</span><span class="o">=</span>host <span class="nt">--gpus</span> all carlasim/carla:0.9.14 /bin/bash ./CarlaUE4.sh <span class="nt">-opengl</span> <span class="nt">-world-port</span><span class="o">=</span>2000 <span class="nt">-RenderOffScreen</span>
4.26.2-0+++UE4+Release-4.26 522 0
Disabling core dumps.
sh: 1: xdg-user-dir: not found
ALSA lib confmisc.c:767:<span class="o">(</span>parse_card<span class="o">)</span> cannot find card <span class="s1">'0'</span>
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_card_driver returned error: No such file or directory
ALSA lib confmisc.c:392:<span class="o">(</span>snd_func_concat<span class="o">)</span> error evaluating strings
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_concat returned error: No such file or directory
ALSA lib confmisc.c:1246:<span class="o">(</span>snd_func_refer<span class="o">)</span> error evaluating name
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_refer returned error: No such file or directory
ALSA lib conf.c:5007:<span class="o">(</span>snd_config_expand<span class="o">)</span> Evaluate error: No such file or directory
ALSA lib pcm.c:2495:<span class="o">(</span>snd_pcm_open_noupdate<span class="o">)</span> Unknown PCM default
ALSA lib confmisc.c:767:<span class="o">(</span>parse_card<span class="o">)</span> cannot find card <span class="s1">'0'</span>
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_card_driver returned error: No such file or directory
ALSA lib confmisc.c:392:<span class="o">(</span>snd_func_concat<span class="o">)</span> error evaluating strings
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_concat returned error: No such file or directory
ALSA lib confmisc.c:1246:<span class="o">(</span>snd_func_refer<span class="o">)</span> error evaluating name
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_refer returned error: No such file or directory
ALSA lib conf.c:5007:<span class="o">(</span>snd_config_expand<span class="o">)</span> Evaluate error: No such file or directory
ALSA lib pcm.c:2495:<span class="o">(</span>snd_pcm_open_noupdate<span class="o">)</span> Unknown PCM default
ALSA lib confmisc.c:767:<span class="o">(</span>parse_card<span class="o">)</span> cannot find card <span class="s1">'0'</span>
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_card_driver returned error: No such file or directory
ALSA lib confmisc.c:392:<span class="o">(</span>snd_func_concat<span class="o">)</span> error evaluating strings
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_concat returned error: No such file or directory
ALSA lib confmisc.c:1246:<span class="o">(</span>snd_func_refer<span class="o">)</span> error evaluating name
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_refer returned error: No such file or directory
ALSA lib conf.c:5007:<span class="o">(</span>snd_config_expand<span class="o">)</span> Evaluate error: No such file or directory
ALSA lib pcm.c:2495:<span class="o">(</span>snd_pcm_open_noupdate<span class="o">)</span> Unknown PCM default
ALSA lib confmisc.c:767:<span class="o">(</span>parse_card<span class="o">)</span> cannot find card <span class="s1">'0'</span>
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_card_driver returned error: No such file or directory
ALSA lib confmisc.c:392:<span class="o">(</span>snd_func_concat<span class="o">)</span> error evaluating strings
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_concat returned error: No such file or directory
ALSA lib confmisc.c:1246:<span class="o">(</span>snd_func_refer<span class="o">)</span> error evaluating name
ALSA lib conf.c:4528:<span class="o">(</span>_snd_config_evaluate<span class="o">)</span> <span class="k">function </span>snd_func_refer returned error: No such file or directory
ALSA lib conf.c:5007:<span class="o">(</span>snd_config_expand<span class="o">)</span> Evaluate error: No such file or directory
ALSA lib pcm.c:2495:<span class="o">(</span>snd_pcm_open_noupdate<span class="o">)</span> Unknown PCM default
</code></pre></div></div> <h3 id="xdg-user-dir-not-found-error">“xdg-user-dir: not found” Error</h3> <p>When running CARLA, especially in Docker environments, you might encounter the <code class="language-plaintext highlighter-rouge">xdg-user-dir: not found</code> error. This error typically occurs because the <code class="language-plaintext highlighter-rouge">xdg-user-dirs</code> package, which is responsible for managing user directories like Downloads, Desktop, etc., is not installed or accessible in the container. While this error generally does not impact the functionality of CARLA if you’re not using these directories (see <a href="https://github.com/carla-simulator/carla/issues/3514">this issue</a>).</p>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Tutorials"/><summary type="html"><![CDATA[This guide will walk you through the process of setting up the CARLA simulator (version 0.9.14) using Docker.]]></summary></entry><entry><title type="html">Coding week1 5/27-6/02</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/coding-week1-5-27-6-02/" rel="alternate" type="text/html" title="Coding week1 5/27-6/02"/><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week1</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/coding-week1-5-27-6-02/"><![CDATA[<h3 id="weekly-meeting">Weekly Meeting</h3> <p>In this week’s meeting, we reviewed the project’s current progress. I updated the project blog and asked for feedback. I have successfully set up CARLA on Docker, with plans to transition to a physical machine soon, and I will post a Docker installation tutorial later. We discussed several technical issues, including dependencies and model loading errors, and discussed data collection script problems in relation to graphical mode and ROS Bridge compatibility.</p> <p>Open issues on GitHub were reviewed, with a particular note on the need to make a PR and future plans for Docker installation. During the open floor discussion, the team discussed the potential for reproducibility and future enhancements similar to the LMdrive model. Concerns about GPU resources and API token support from Google were raised, with plans to inquire further.</p> <p>More details can be found here: <a href="https://docs.google.com/document/d/1b2ZEU5Gt8gP2ae_YzNSJSd7RukUrsG_aDJFLnbvoQiM/edit">Google Doc</a></p> <h3 id="to-do-list">To-Do List</h3> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Check dependencies and model loading error</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Conduct a literature review about the potential direction</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>A CARLA docker installation documentation</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Inquiry about potential GPU resources and API token support</li> </ul> <h3 id="following-previous-issues"><strong>Following previous issues</strong></h3> <p>Following previous issues: <a href="https://github.com/TheRoboticsClub/gsoc2023-Meiqi_Zhao/issues/2">issue1</a> and <a href="https://github.com/TheRoboticsClub/gsoc2023-Meiqi_Zhao/issues/3">issue2</a>, the model’s issues were resolved by changing the number of outputs, and the environment dependency issues have also been resolved. However, there are still problems with the current data collection, but an issue has not yet been raised; it is still being checked. The current data collection scripts may encounter errors or pauses, which could require manual intervention or result in delays in data collection. There are several solutions currently available:</p> <ol> <li>Use a physical machine to set up checks for cameras, etc., in a graphical interface.</li> <li>LMDrive has provided some scripts for data collection that can be tested and explored.</li> </ol> <h3 id="literature-review"><strong>Literature review</strong></h3> <h4 id="modules">Modules</h4> <p>The architectural framework of world models is structured to facilitate complex decision-making processes that closely emulate human cognitive functions. These models are comprised of several distinct but interconnected modules, each serving a crucial role in the system’s overall performance and capability:</p> <ul> <li><strong>Perception:</strong> This module serves as the interface between the external environment and the model, capturing multi-modal sensory inputs such as images, sounds, and tactile feedback. By transforming raw sensory data into a more digestible and actionable format, this module ensures that the model is well-equipped to respond to the complexities of its surroundings. It leverages advanced sensory technologies and encoding mechanisms. This module is adept not only at recognizing objects or scenes but also at interpreting the structural and contextual relationships within the sensory information. This precise environmental understanding forms the foundation for all subsequent data-driven decision-making processes.</li> <li><strong>Memory Module:</strong> This module serves a role akin to the human hippocampus, crucial for encoding, storing, and retrieving information from past and present environmental interactions. It supports both short-term and long-term memory functionalities. Short-term memory processes immediate tasks—such as remembering specific details of a traffic intersection, while long-term memory retains the outcomes of particular events or the efficacy of complex strategies over time. By managing a dynamic repository of experiences, this module enables continuous learning and adaptation, which are indispensable for responding to evolving challenges.</li> <li><strong>Action Module:</strong> This module operates based on the information processed by the Perception and Modules to formulate and execute decisions. It evaluates the current conditions along with forward-looking predictions to develop actions aimed at achieving specific objectives, such as optimizing resource use or maximizing operational efficacy. Typically, this process relies on RL, aiming to maximize rewards. The capability of this module to integrate information and implement strategic decisions is crucial for responsive interactions with the environment, ensuring that the system can adapt to changes effectively and execute tasks efficiently.</li> <li><strong>World Model Module:</strong> Situated at the heart of the system’s architecture, its primary function is to refine and enhance the system’s understanding of the environment by generating comprehensive simulations. Through these simulations, the module projects future environmental conditions, providing a foresight that is critical for strategic planning. This predictive capability allows the system to prepare for potential scenarios, offering a degree of anticipatory adaptation and flexibility.</li> </ul> <p>These modules form an integrated framework that enables world models to simulate human-like cognitive processes and decision-making. This module structure not only enhances the operational capabilities of such systems but also contributes to their ability to operate independently and efficiently in a variety of real-world applications.</p> <h4 id="architectures">Architectures</h4> <p>The architecture of world models is designed to predict future states of environments by balancing deterministic forecasts with the uncertainty of real-world dynamics. In high-dimensional sensory input scenarios, the challenge lies in efficiently representing observed information through latent dynamical models to make compact and accurate predictions. To manage these complexities, a variety of architectures have been proposed, including the RSSM and the JEPA, as well as Transformer-based architecture.</p> <ul> <li><strong>RSSM:</strong> The RSSM stands at the forefront of this architectural evolution, designed to efficiently navigate and predict within latent spaces. By decomposing the latent state into deterministic and stochastic elements, RSSM manages the unpredictable nature of real-world environments. This model excels in continuous control tasks by learning dynamic environmental models from sensory data like pixels and formulating action plans within the encoded latent space. The RSSM features a dual-path architecture where its deterministic components provide stability and its stochastic components enhance adaptability. This structure makes RSSM ideal for scenarios that require robust yet flexible predictive capabilities.</li> <li><strong>JEPA:</strong> JEPA revolutionizes predictive modelling by focusing on a higher-level representation space rather than traditional pixel-level output generation. The architecture’s predictive process involves generating and utilizing latent variables to fill in gaps or predict missing elements in the input data. This architecture marks a paradigm shift by abstracting inputs and targets through dual encoders into representations and leveraging a latent variable for prediction. JEPA excels in filtering out noise and irrelevancies, concentrating on essential data elements. Its use of self-supervised learning enables pre-training on unlabeled datasets, refining predictive accuracy for both visual and non-visual tasks.</li> <li><strong>Transformer-based Architecture:</strong> Leveraging the attention mechanism inherent in Transformer architectures, these models provide a framework for handling memory-intensive tasks. Transformer-based world models like the Spatial Temporal Patchwise Transformer (STPT) and the Transformer State Space Model (TSSM) focus on different segments of input data simultaneously. These models excel at managing intricate temporal and spatial dependencies. This capability enables effective management and prediction of dynamic environmental interactions through their advanced memory access and dependency tracking.</li> </ul> <p><em>Please note that there is also a section of review content that will be part of a paper to be submitted and will be expected to make public next week.</em></p> <h2 id="gpu-resource">GPU resource</h2> <p>Assessing the risks, it’s clear that we should rely on external sources like university clusters, especially when I need consistent access to high-performance GPUs such as the NVIDIA A100. But I’ve faced challenges with availability. While I can access 30 series and A4000 GPUs, I’m also exploring potential access through a university cluster. Unfortunately, GSOC has confirmed they cannot provide my GPU resources. I’m also considering GPU resources from the University of Edinburgh, though this might require queuing for access. Sergio mentioned that we have access to a powerful GPU cluster at his university, and they might give me access when needed.</p> <h2 id="docker-installation">Docker installation</h2> <p>We have updated the step-by-step tutorial for installing CARLA based on Docker. Unlike the official website, this version includes more detailed troubleshooting steps. More details can be find in this <a href="/gsoc2024-ZebinHuang/blog/2024/carla-docker/">post</a>.</p>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[Weekly Meeting]]></summary></entry><entry><title type="html">Bonding 5/21 - 5/27</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/community-bonding-5-21-5-27/" rel="alternate" type="text/html" title="Bonding 5/21 - 5/27"/><published>2024-05-26T00:00:00+00:00</published><updated>2024-05-26T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/community-bonding</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/community-bonding-5-21-5-27/"><![CDATA[<h3 id="weekly-meeting">Weekly Meeting</h3> <p>During the May 20, 2024, we discussed beginning the project by replicating last year’s model using a simple LLM for handling different input commands and gradually progressing towards more complex models. Key tasks for moving forward include conducting a literature review to define the project’s specific research question, setting up necessary tools like CARLA and behavior metrics, and addressing technical setup challenges.</p> <p>More details can be found here: <a href="https://docs.google.com/document/d/1b2ZEU5Gt8gP2ae_YzNSJSd7RukUrsG_aDJFLnbvoQiM/edit">Google Doc</a></p> <h3 id="to-do-list-during-the-bonding-period">To-Do List during the bonding period</h3> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Set up a blog based on examples from previous years.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Set up CARLA.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Run Qi’s models.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Read and analyze literature on autonomous driving and LLMs.</li> </ul> <h3 id="code-replication"><strong>Code Replication</strong></h3> <p>This week, I attempted to replicate certain elements of the project codebase <a href="https://github.com/TheRoboticsClub/gsoc2023-Meiqi_Zhao">Meiqizhao’s code</a> and encountered some challenges that required raising issues for resolution. Specifically, I opened two issues <a href="https://github.com/TheRoboticsClub/gsoc2023-Meiqi_Zhao/issues/2">issue1</a> <a href="https://github.com/TheRoboticsClub/gsoc2023-Meiqi_Zhao/issues/3">issue2</a> and <a href="https://github.com/TheRoboticsClub/gsoc2023-Meiqi_Zhao/pull/4">one PR</a> regarding bugs found during the replication process. To enhance reproducibility, I am currently working with Docker, and I also plan to provide a Docker branch later on.</p> <h3 id="behavior-metrics-exploration"><strong>Behavior Metrics Exploration</strong></h3> <p>I reviewed the <a href="https://github.com/JdeRobot/BehaviorMetrics">Behaviour Metrics</a> repos and related papers. The Behavior Metrics can provide a structured framework for quantifying the effectiveness and performance of autonomous system in simulated scenarios. Incorporating text input for autonomous driving guidance enhances the Behavior Metrics benchmark by interactivity and interpretability. Here are some potential integration methods and benefits:</p> <ol> <li><strong>Expanded Testing Scenarios</strong>: It enables the creation of a broader range of test environments and situations that include verbal commands and interactions.</li> <li><strong>Enhanced Textual Interpretability</strong>: Provides clarity on how the system interprets and responds to natural language inputs, which improves the system’s transparency and trustworthiness.</li> <li><strong>Adapted Interaction Methods</strong>: Allows for modifications in user interaction, offering more intuitive and accessible ways for users to communicate with autonomous systems.</li> </ol> <h3 id="literature-review-and-feasibility-analysis"><strong>Literature Review and Feasibility Analysis</strong></h3> <p>I conducted a review on research papers related to our project. The focus was on assessing the feasibility of replicating the studies, considering factors like data availability, computational requirements, and whether the methods are open-source. Such analysis helps in understanding the practical aspects of implementing these research findings in our work.</p> <table> <thead> <tr> <th>Paper Title</th> <th>Reproducibility</th> <th>Data Volume</th> <th>Technical Difficulty</th> <th>GPU Requirements</th> </tr> </thead> <tbody> <tr> <td>GPT-4V Takes the Wheel <d-cite key="huangGPT4VTakesWheel2024a"></d-cite></td> <td>Low: Uses publicly available datasets. <span style="color:red;">Not open-sourced</span></td> <td>JAAD, WiDEVIEW</td> <td>High: Integrates vision and language models for dynamic behavior prediction</td> <td>High: VLM processing but not illustrated</td> </tr> <tr> <td>Driving with LLMs <d-cite key="chenDrivingLLMsFusing2023a"></d-cite></td> <td>Low: New dataset and unique architecture, reproducibility <a href="https://github.com/wayveai/Driving-with-LLMs">GitHub</a></td> <td>Custom 160k QA pairs, 10k driving scenario. Which simulator?</td> <td>Very High: Novel fusion of vector modalities and LLMs</td> <td>Moderate: Minimum of 20GB VRAM for running evaluations, Minimum of 40GB VRAM for training</td> </tr> <tr> <td>LMDrive <d-cite key="shaoLMDriveClosedLoopEndtoEnd2023a"></d-cite></td> <td>High: Dataset and models are open-sourced, but complexity in GPU setup</td> <td><span style="color:green;">64K parsed clips and 464K notice instructions</span></td> <td>Very High: Real-time, closed-loop control with LLMs in vehicles</td> <td><span style="color:red;">Very High: 2~3 days for the visual encoder on 8x A100 (80G)</span></td> </tr> <tr> <td>Language Models as Trajectory Generators <d-cite key="kwonLanguageModelsZeroShot2023"></d-cite></td> <td>High: Standard dataset, clear methodology and evaluation process</td> <td>Flexible data generation with Pybullet</td> <td>Moderate: Focus on trajectory generation using LLMs, less complex than real-time control systems</td> <td>Low: Less demanding compared to real-time visual tasks</td> </tr> </tbody> </table> <p></p> <p>Here is a summary of the preliminary analysis of different literature pieces:</p> <ol> <li><strong>GPT-4V Takes the Wheel</strong>: This work utilizes publicly available datasets but is not open-sourced, which poses a significant barrier to reproducibility. Although it can serve as a conceptual reference, the lack of open access means it cannot be directly replicated.</li> <li><strong>Driving with LLMs</strong>: The source code is open. However, the simulator used is proprietary to Wayve, restricting access and thus full replication of the project. While the architecture and approach can be studied.</li> <li><strong>LMDrive</strong>: This project appears the most promising in terms of openness and practical usability. It is conducted on the Carla simulator platform, and pre-trained models along with the dataset are provided. Although there are no current reproducibility issues or bugs reported, the main challenge is the significant computational requirement—training requires eight A100 GPUs (80GB each). Initial testing might focus on evaluating the provided pre-trained models due to these resource demands.</li> <li><strong>Language Models as Trajectory Generators</strong>: This work offers a unique perspective by using zero-shot methods in manipulators, which is the least resource-intensive approach among the ones listed. However, for real-time systems like autonomous driving, this approach would need to incorporate more robust and safer control mechanisms to ensure reliability and safety in dynamic environments.</li> </ol> <p>From the feasibility standpoint, some of the literature reviewed indicated very high resource requirements, such as one paper necessitating 8 * A100 GPUs. These are substantial resource demands that pose challenges for replication.</p> <p>The core question we need to address is: What is our objective? If the goal is to replicate existing solutions and integration, we need to identify the features and MVP. However, if our aim is to optimize, the biggest hurdle is the training phase, particularly the GPU bottlenecks during this process. This will need to be discussed further in next week’s meeting.</p> <h3 id="moving-forward"><strong>Moving Forward</strong></h3> <p>Understanding these resource limitations and objectives will help guide our project’s direction. Our next steps involve deciding whether to seek resource optimization or to focus on adapting our goals to fit the available computational resources. Additionally, we are currently addressing several issues and plan to conduct further literature research to deepen my understanding of the field.</p> ]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[Analysis of code replication and literature review]]></summary></entry><entry><title type="html">Bonding 5/15 - 5/21</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/community-bonding-5-15-5-21/" rel="alternate" type="text/html" title="Bonding 5/15 - 5/21"/><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/community-bonding</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/community-bonding-5-15-5-21/"><![CDATA[<h3 id="gsoc-setting-things">GSoC Setting Things</h3> <p>Completed initial setup tasks for GSoC 2024, including payment registration, joining the Discord channel, and participating in the May 7th Contributor Summit. The summit was a great opportunity to connect with developers worldwide and gain valuable insights and tips for the program. Engaged with my mentor and community, reviewed roles and responsibilities, updated my display name, and logged meetings for future reference.</p> <h3 id="meeting-with-mentors-59">Meeting with Mentors (5/9)</h3> <p>During our first meeting, I had the opportunity to get acquainted with my mentors and discuss the project’s initial steps. Apoorv shared his journey from being a GSoC contributor to becoming a mentor at JdeRobot. David, a first-year mentor at GSoC and a PhD student, provided insights into his research on vision-based detection and drone localization.</p> <p>We discussed the technical setup for the project, including GPU access. Communication was emphasized, with Slack being the primary tool for ongoing discussions. Apoorv explained the usage of our current repository, which mainly stores documents and blogs.</p> <p>The discussion also covered the project’s initial steps, such as reviewing relevant research papers and developing a minimum viable product. Apoorv provided instructions for setting up the Carla simulator, which will be essential for our project.</p> <p>Sergio suggested starting with a simple BERT model that classifies instructions into commands, using Qi’s development from last year’s project as a starting point. We also discussed the possibility of publishing our results in a scientific paper after the summer, which added an exciting goal to our project.</p> <p>More details can be found here: <a href="https://docs.google.com/document/d/1b2ZEU5Gt8gP2ae_YzNSJSd7RukUrsG_aDJFLnbvoQiM/edit">Google Doc</a></p> <p>This document is intended to help everyone from different time zones stay on track for past meetings. It will also streamline our preparations for future meetings. I will make and review the agendas before each meeting.</p> <h4 id="to-do-list-during-the-bonding-period">To-Do List during the bonding period</h4> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Set up a blog based on examples from previous years.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Set up CARLA.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Run Qi’s models.</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Read and analyze literature on autonomous driving and LLMs.</li> </ul> <h3 id="jderobot-kick-off-meeting-515">JdeRobot Kick-off Meeting (5/15)</h3> <p>In this meeting, we were welcomed to the JdeRobot GSoC program for 2024. A brief presentation about JdeRobot and its main projects was given, which helped in understanding the context and scope of the organization. Each participant, including myself, gave a quick introduction and described their project.</p> <h3 id="progress">Progress</h3> <p>Last week, I focused on setting up my blog and configuring the Carla simulator using Docker. I created the blog using Jekyll and <a href="https://github.com/alshedivat/al-folio/tree/master">al-folio</a>, referencing examples from previous years, and successfully configured the deployment workflow.</p> <p>For the Carla simulator, I opted for a Docker setup, considering I am currently primarily using servers and macOS. This choice was made to simplify the initial setup process, with plans to transition to a physical machine later. The documentation available online was quite scattered, and there were differences from the official tutorials. Hence, I plan to write a separate blog later to document this setup process in detail.</p> <p>Additionally, I have been reviewing <a href="https://github.com/TheRoboticsClub/gsoc2023-Meiqi_Zhao">Meiqizhao’s code</a> implementation and understanding the general ideas behind <a href="https://github.com/JdeRobot/BehaviorMetrics">behaviour metrics</a>.</p> <p>Moving forward, I will continue with the setup tasks and delve deeper into the research question to ensure a solid foundation for our project.</p>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="GSoC,"/><category term="Weekly_blogs,"/><category term="JdeRobot"/><summary type="html"><![CDATA[Initial setup for GSoC 2024 and summary of first meetings]]></summary></entry></feed>