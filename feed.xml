<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/feed.xml" rel="self" type="application/atom+xml"/><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-05T14:44:08+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/feed.xml</id><title type="html">Zebin Huang | JdeRobot x GSoC2024</title><subtitle>Zebin Huang | JdeRobot x GSoC2024 </subtitle><entry><title type="html">GSoC 2024 Project Recap</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/final/" rel="alternate" type="text/html" title="GSoC 2024 Project Recap"/><published>2024-10-30T00:00:00+00:00</published><updated>2024-10-30T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/final</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/final/"><![CDATA[<h1 id="end-to-end-autonomous-vehicle-driving-based-on-text-based-instructions">End-to-End Autonomous Vehicle Driving Based on Text-Based Instructions</h1> <div style="text-align: center;"> Mentors: <a href="mailto:sergiopaniegoblanco@gmail.com">Sergio Paniego Blanco</a>, <a href="mailto:apoorvgarg.ms@gmail.com">Apoorv Garg</a>, <a href="mailto:nikhil.paliwal14@gmail.com">Nikhil Paliwal</a>, <a href="mailto:david.perez.saura@upm.es">David Pérez</a>, and <a href="mailto:skyler.zhaomeiqi@gmail.com">Meiqi Zhao</a> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/final/jde_gsoc-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/final/jde_gsoc-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/final/jde_gsoc-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/final/jde_gsoc.png" class="img-fluid rounded w-30 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p> <a href="https://github.com/TheRoboticsClub/gsoc2024-ZebinHuang">GitHub</a> | <a href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/">Website</a> | <a href="https://youtu.be/_0CpMj_G5fM">Video</a> | <a href="https://docs.google.com/document/d/1b2ZEU5Gt8gP2ae_YzNSJSd7RukUrsG_aDJFLnbvoQiM/edit?tab=t.0#heading=h.isdep2i97rn2">Meeting Logs</a> | <a href="https://huggingface.co/zebin-huang/gsoc2024-ZebinHuang">Model Card</a> | <a href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/">Data Card</a> </p> </div> <h2 id="summary">Summary</h2> <p>The 2024 GSoC project focused on the development of an end-to-end system in CARLA, built upon multimodal large models and a modified PilotNet architecture. Specifically, the agent component translates user inputs provided as natural language commands into structured control signals, including target distance and high-level control (HLC). These signals are processed by the control model, which uses distance-aware metrics to guide vehicle behavior. Furthermore, a Streamlit-based <a href="https://gsoc24-zebinhuang.streamlit.app/">web app</a> was developed to streamline interaction and training by providing an interface for generating and training user commands with LLMs. Finally, the entire system was tested in CARLA, where the generated control signals were applied to a simulated vehicle.</p> <p>I would like to thank Google Summer of Code (GSoC), JdeRobot, and everyone who supported me throughout this project for the opportunity to contribute to the open-source community. I am especially grateful to my mentors for their guidance, particularly Sergio Paniego Blanco, whose insights and guidance were instrumental to my progress.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/final/framework-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/final/framework-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/final/framework-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/final/framework.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <iframe width="700" height="500" src="https://www.youtube.com/embed/_0CpMj_G5fM" title="2024 GSoC" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <h2 id="weekly-blog">Weekly Blog</h2> <h4 id="week-21"><a href="/gsoc2024-ZebinHuang/blog/2024/week21/">Week 21</a></h4> <p>We concluded the model setup by incorporating all the components into one model to test and evaluate.</p> <h4 id="week-20"><a href="/gsoc2024-ZebinHuang/blog/2024/week20/">Week 20</a></h4> <p>In Week 20, we improved CARLA’s vehicle control system by integrating distance-aware metrics. This update required refining data collection to track new metrics. A proof of concept showed successful action-switching based on distance, with plans to analyze datasets using HDF5 and scale up model training.</p> <h4 id="week-19"><a href="/gsoc2024-ZebinHuang/blog/2024/week19/">Week 19</a></h4> <p>During Week 19, we parsed CARLA’s Town02 map in OpenDRIVE format, generating structured test cases with start/end points. </p> <h4 id="week-18"><a href="/gsoc2024-ZebinHuang/blog/2024/week18/">Week 18</a></h4> <p>In Week 18, we introduced a distance-aware control method in CARLA to refine autonomous actions based on distance traveled, developed prompts for distance-specific commands, and validated real-time distance measurement in Town01.</p> <h4 id="week-17"><a href="/gsoc2024-ZebinHuang/blog/2024/week17/">Week 17</a></h4> <p>In this week, we explored integrating Visual Question Answering with reinforcement learning to enhance autonomous driving.</p> <h4 id="weeks-15-16"><a target="_blank" rel="noopener noreferrer" href="/gsoc2024-ZebinHuang/blog/2024/week1516/">Weeks 15-16</a></h4> <p>In Week 15 and 16, we explored ideas to enhance autonomous vehicle capabilities through natural language instructions, particularly for complex, multi-turn interactions. Key efforts included compiling a list of potential research directions, assessing technical feasibility.</p> <h4 id="week-14"><a href="/gsoc2024-ZebinHuang/blog/2024/week14/">Week 14</a></h4> <p>In Week 14, we developed an "Ideas List" to explore potential research paths and merged new app updates into the main branch, including model testing in CARLA and a Streamlit web app. Documentation was refreshed, and recent progress was shared on social media.</p> <h4 id="weeks-12-13"><a href="/gsoc2024-ZebinHuang/blog/2024/week1213/">Weeks 12-13</a></h4> <p>For the Weeks of 12 and 13, we focused on improving model evaluation and setting future research goals. Key tasks included enhancing model accuracy, addressing deployment issues, and preparing a social media update with a refined demo video.</p> <h4 id="weeks-10-11"> <a href="/gsoc2024-ZebinHuang/blog/2024/coding-week8-7-29-8-11/">Weeks 10-11</a></h4> <p>In Week 10 and 11, we focused on preparing for the mid-term evaluation, improving dataset generation, and building a Streamlit app to streamline model training and analysis. Key challenges included managing model storage and refining evaluation techniques.</p> <h4 id="week-9"><a href="/gsoc2024-ZebinHuang/blog/2024/coding-week8-7-22-7-28/">Week 9</a></h4> <p>This week’s update covers progress in using LLMs to generate training data, training a BERT model, and integrating it into CARLA to improve the simulator's response to human instructions.</p> <h4 id="week-8"><a href="/gsoc2024-ZebinHuang/blog/2024/coding-week8-7-15-7-21/">Week 8</a></h4> <p>This week focused on improving model performance, and flexibility. Key updates included setting up CARLA with a graphical interface, optimizing model training and refactoring the codebase.</p> <h4 id="week-7"><a href="/gsoc2024-ZebinHuang/blog/2024/coding-week7-7-08-7-14/">Week 7</a></h4> <p>We then added command-based controls and tested various configurations of the model for improving its accuracy. We were trying to fine-tune the responsiveness of this model in various conditions.</p> <h4 id="week-6"><a href="/gsoc2024-ZebinHuang/blog/2024/coding-week345-7-01-7-07/">Week 6</a></h4> <p>We have worked this week on fine-tuning the training to improve its efficiency and get early accuracy. Tuning the parameters contributed to making the process of training more stable, which is a very positive phenomenon to witness.</p> <h4 id="weeks-3-5"><a href="/gsoc2024-ZebinHuang/blog/2024/coding-week345-6-10-6-30/">Weeks 3-5</a></h4> <p>In these two weeks, we focused on model refinement and optimized the collection of data. We worked to smoothen the workflows for higher efficiency and higher accuracies in our data.</p> <h4 id="week-2"><a href="/gsoc2024-ZebinHuang/blog/2024/coding-week2-6-03-6-09/">Week 2</a></h4> <p>We developed improved data generation techniques and set up initial model training. <h4 id="week-1"><a href="/gsoc2024-ZebinHuang/blog/2024/coding-week1-5-27-6-02/">Week 1</a></h4> <p>During this week, we set up the skeleton of the project along with initial data pipelines and prepared the ground so that core development tasks could be rolled out.</p> <h4 id="community-bonding-week-2"><a href="/gsoc2024-ZebinHuang/blog/2024/community-bonding-5-21-5-27/">Community Bonding Week 2</a></h4> <p>Our attention was placed on planning milestones. The coherence in the community brought better understanding of the roles to be played within the project in the coming stages.</p> <h4 id="community-bonding-week-1"><a href="/gsoc2024-ZebinHuang/blog/2024/community-bonding-5-14-5-20/">Community Bonding Week 1</a></h4> <p>The blog website setup, commencement of literature research, and laying the backbone for the project were some of the initial works. This included resource organization and initial documentation in preparation for the project kickoff.</p> <h3>Pull Requests</h3> <ul> <li><a href="https://github.com/TheRoboticsClub/gsoc2024-ZebinHuang/pull/6">#6 feat(carla_llms): distance-aware control with LLMs planning</a></li> <li><a href="https://github.com/TheRoboticsClub/gsoc2024-ZebinHuang/pull/5">#5 feat(streamlit-demo): complete Streamlit integration and merge with main</a></li> <li><a href="https://github.com/TheRoboticsClub/gsoc2024-ZebinHuang/pull/4">#4 feat(streamlit-demo): add Streamlit App</a></li> <li><a href="https://github.com/TheRoboticsClub/gsoc2024-ZebinHuang/pull/3">#3 feat(carla): add CARLA integration</a></li> <li><a href="https://github.com/TheRoboticsClub/gsoc2024-ZebinHuang/pull/2">#2 Add Streamlit Demo, Fix Deployment Issues</a></li> </ul> </p>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs,"/><category term="Final_report"/><summary type="html"><![CDATA[End-to-End Autonomous Vehicle Driving Based on Text-Based Instructions]]></summary></entry><entry><title type="html">Coding week21 10/21-10/27</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week21/" rel="alternate" type="text/html" title="Coding week21 10/21-10/27"/><published>2024-10-21T00:00:00+00:00</published><updated>2024-10-21T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week21</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week21/"><![CDATA[<div style="background-color: #f9f9f9; padding: 10px; border-left: 4px solid #007acc; font-size: 1em; color: #333; margin-bottom: 20px;"> This blog was previously documented in LaTeX, so some formatting discrepancies might occur during the migration. You can view the PDF source file via <a href="https://www.overleaf.com/read/sccyqkkkkmcb#8c0024" style="color: #007acc; font-weight: bold;">Overleaf link</a>. </div> <p>This week, we focused on the integration of new command functionalities. We introduced a distance-based command to help the model learn state transitions. This post outlines the modifications made to the <code class="language-plaintext highlighter-rouge">PilotNetOneHot</code> model to incorporate a new command <code class="language-plaintext highlighter-rouge">Run50</code> an additional input, <code class="language-plaintext highlighter-rouge">distance</code>, which is a vector of size 1 range within (0,50) meters. Key changes were made in the model’s architecture.</p> <h3 id="data-collection">Data Collection</h3> <p>This section explains how the data collection process integrates distance commands.</p> <p>The <code class="language-plaintext highlighter-rouge">distance</code> vector is a critical part of the dataset as it determines how the vehicle behaves in response to the target distance. Specifically, when the vehicle is within a 50-meter range of a stopping point, the corresponding control commands will reflect a reduction in throttle and an increase in braking, teaching the model to stop the vehicle.</p> <p>When the vehicle reaches or exceeds a distance of 50 meters from the target, the data reflects a command to stop. During training, the model will learn to replicate this behaviour, applying the brake and reducing the throttle when the vehicle is within 50 meters of the stopping point.</p> <p>For example, during data collection, when the distance reaches 50 meters, the control signals recorded will show a reduction in throttle and an increase in braking:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">distance</span> <span class="o">=</span> <span class="mf">50.0</span>
<span class="n">controls</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">throttle</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>  <span class="c1"># Reduce throttle to 0 to stop the vehicle
</span>    <span class="sh">'</span><span class="s">brake</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1.0</span>      <span class="c1"># Apply brake to stop the vehicle
</span><span class="p">}</span>
</code></pre></div></div> <p>In the collected data, the “Run50” command appears infrequently. Therefore, we extracted the segments containing this command for model training.</p> <h3 id="model-initialization">Model Initialization</h3> <p>The structure of the <code class="language-plaintext highlighter-rouge">PilotNetOneHot</code> model remains unchanged. The key change is in the fully connected layers, where we account for the additional <code class="language-plaintext highlighter-rouge">distance</code> input. The name for the new model is PilotNetOneHotDistance.</p> <p>Since we are adding a <code class="language-plaintext highlighter-rouge">distance</code> input with a size of 50, the updated initialization becomes:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">fc_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">8</span> <span class="o">*</span> <span class="mi">35</span> <span class="o">*</span> <span class="mi">24</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">num_hlc</span> <span class="o">+</span> <span class="n">num_light</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
</code></pre></div></div> <p>In the original model, the forward pass handled the concatenation of the image features, speed, HLC, and traffic light inputs. Now, we modify the forward pass to include the <code class="language-plaintext highlighter-rouge">distance</code> as well.</p> <p>This modification allows the model to process the <code class="language-plaintext highlighter-rouge">distance</code> input along with the existing features. The <code class="language-plaintext highlighter-rouge">distance</code> input is concatenated with the flattened image features, speed, HLC, and traffic light data before being passed through the fully connected layers.</p> <p>The updated forward pass in the training loop becomes:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
    <span class="n">img</span><span class="p">,</span> <span class="n">speed</span><span class="p">,</span> <span class="n">hlc</span><span class="p">,</span> <span class="n">light</span><span class="p">,</span> <span class="n">distance</span><span class="p">,</span> <span class="n">controls</span> <span class="o">=</span> <span class="n">data</span>  <span class="c1"># Include distance
</span>    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">speed</span> <span class="o">=</span> <span class="n">speed</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">hlc</span> <span class="o">=</span> <span class="n">hlc</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">light</span> <span class="o">=</span> <span class="n">light</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="n">distance</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">controls</span> <span class="o">=</span> <span class="n">controls</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Forward pass
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">speed</span><span class="p">,</span> <span class="n">hlc</span><span class="p">,</span> <span class="n">light</span><span class="p">,</span> <span class="n">distance</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">controls</span><span class="p">)</span>

    <span class="c1"># Backward pass and optimization
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <p>In the video, the final distance is set to 44 meters. This could be because, within the 0–50 meter training, the model encounters situations like waiting at traffic lights or stopping for obstacles. Such instances may reduce the model’s ability to learn precise control.</p> <iframe width="700" height="500" src="https://www.youtube.com/embed/LZzG00QXZI8" title="Run50 44m" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p>We also tested switching to a high-level command. In the following video, you can see an action transition, although there is a delay when restarting. This delay might occur because, during data collection, distinct states are triggered, and the model has mostly learned the stop command. As a result, after executing the <code class="language-plaintext highlighter-rouge">Run50</code> command, the model defaults to a stop signal, needing extra time to switch to the next high-level command.</p> <iframe width="700" height="500" src="https://www.youtube.com/embed/9egxvlqG-0k" title="Run50 LaneFollow" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[Modifications to PilotNetOneHot model to incorporate distance-aware control]]></summary></entry><entry><title type="html">Coding week20 10/14-10/20</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week20/" rel="alternate" type="text/html" title="Coding week20 10/14-10/20"/><published>2024-10-14T00:00:00+00:00</published><updated>2024-10-14T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week20</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week20/"><![CDATA[<div style="background-color: #f9f9f9; padding: 10px; border-left: 4px solid #007acc; font-size: 1em; color: #333; margin-bottom: 20px;"> This blog was previously documented in LaTeX, so some formatting discrepancies might occur during the migration. You can view the PDF source file via <a href="https://www.overleaf.com/read/sccyqkkkkmcb#8c0024" style="color: #007acc; font-weight: bold;">Overleaf link</a>. </div> <h3 id="introduction">Introduction</h3> <p>This week, the vehicle control system in CARLA was improved by introducing distance-aware metrics. The primary focus of this week was on extending the control system from being purely action-based to a more robust <em>action-distance aware</em> control.</p> <p>Previously, the vehicle control system was designed to execute driving actions like <em>Left</em>, <em>Right</em>, or <em>Straight</em> based solely on high-level commands. However, this approach lacks flexibility in adjusting the vehicle’s behaviour based on the distance required to perform these actions. To address this, a distance component was integrated into the control system. This means that actions are now tied to specific distances and enable finer control over when and how long actions should be executed.</p> <p>We also focused on refining data collection practices. Key discussions involved improving distance tracking metrics, such as vehicle distance traveled and distance after action initiation, and managing data inconsistencies. I explored using HDF5 suggested by mentors for sampled data analysis.</p> <h3 id="data-collection">Data Collection</h3> <p>To support the new action-distance aware system, we needed to modify the data collection process to gather additional metrics related to distance. These new metrics provide more context and enable the vehicle to make better decisions when controlling its movements. Currently, we are uncertain about the exact metrics that will be used, but to minimize repeated data collection efforts, we aim to propose as many relevant metrics as possible.</p> <p>The following distance-based metrics were introduced:</p> <ul> <li><strong>Distance to next waypoint</strong>: Measures the distance from the vehicle’s current position to the next waypoint.</li> <li><strong>Distance traveled</strong>: Keeps track of the total distance the vehicle has travelled during a route.</li> <li><strong>Distance to stop line</strong>: For traffic light scenarios, this metric measures how far the vehicle is from the stop line when approaching a red light or stop sign.</li> </ul> <h3 id="challenges">Challenges</h3> <p>The data collection environment proved to be unstable. I captured some potential scenarios, as shown in the image (note that red circles indicate collisions). Currently, the data collection process requires manual supervision. A possible solution is to restart the client each time rather than just resetting. This instability required additional time and effort.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week20/failure_cases-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week20/failure_cases-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week20/failure_cases-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/week20/failure_cases.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="action-switch-test-case">Action Switch Test Case</h3> <p>Testing has shown that the current model-based action-switching proof of concept is functioning correctly. Below are two videos demonstrating the results, where high-level actions are switched based on varying distances.</p> <p>Test Case 1</p> <iframe width="700" height="500" src="https://www.youtube.com/embed/048D--M49iM" title="Action Switch 1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p>Test Case 2</p> <iframe width="700" height="500" src="https://www.youtube.com/embed/YmhcweTFlSU" title="Distance Test Case 1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <h3 id="plans-for-next-week">Plans for Next Week</h3> <ul> <li>Dataset Analysis: Start by using the HDF5 package to analyze datasets, beginning with small datasets for initial validation.</li> <li>Test Model Training with Scaled Datasets: Continue testing the model with incrementally larger datasets to validate performance metrics.</li> </ul>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[Enhancing CARLA vehicle control system with distance-aware metrics]]></summary></entry><entry><title type="html">Coding week19 10/07-10/13</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week19/" rel="alternate" type="text/html" title="Coding week19 10/07-10/13"/><published>2024-10-07T00:00:00+00:00</published><updated>2024-10-07T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week19</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week19/"><![CDATA[<div style="background-color: #f9f9f9; padding: 10px; border-left: 4px solid #007acc; font-size: 1em; color: #333; margin-bottom: 20px;"> This blog was previously documented in LaTeX, so some formatting discrepancies might occur during the migration. You can view the PDF source file via <a href="https://www.overleaf.com/read/sccyqkkkkmcb#8c0024" style="color: #007acc; font-weight: bold;">Overleaf link</a>. </div> <p>Building on last week’s insights, we recognized that LLMs alone may not accurately get specific distance, which prompted us to explore alternative methods. This week, we experimented with parsing CARLA’s Town02 map in OpenDRIVE format to automate test case generation. By extracting roads and junctions, we defined structured test cases with precise start and end points, route lengths, and directional commands (Left, Right, Straight). Additionally, we implemented an action-distance aware control approach, pairing each action with a specified distance to enhance the precision of driving commands in simulation.</p> <h3 id="introduction">Introduction</h3> <p>This experiment involves parsing the route map from CARLA (<em>Town02</em> as an example), provided in the OpenDRIVE (<code class="language-plaintext highlighter-rouge">.xodr</code>) format, and generating test cases that define driving scenarios. The map file is available in <a href="https://github.com/carla-simulator/opendrive-test-files/blob/master/OpenDrive/Town02.xodr">Github repo</a>.</p> <p>These test cases include start and end indices, route lengths, and high-level commands such as <em>Left</em>, <em>Right</em>, and <em>Straight</em>.</p> <p>The goal is to automate test case generation based on the map structure, which includes roads, junctions, and their connections. This report details the steps of the experiment, including code, results, and analysis.</p> <h3 id="parsing-opendrive-map">Parsing OpenDRIVE Map</h3> <p>The <em>Town02</em> map file is provided in OpenDRIVE format. This map contains essential elements such as roads and junctions, which define the road network’s topology. The OpenDRIVE file contains:</p> <ul> <li><strong>68 roads</strong>: Each road has an associated ID and length, which defines the road’s geometry.</li> <li><strong>8 junctions</strong>: These represent intersections where multiple roads connect.</li> </ul> <p>The Python code used to parse the OpenDRIVE map file:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">xml.etree.ElementTree</span> <span class="k">as</span> <span class="n">ET</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">ET</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="n">root</span> <span class="o">=</span> <span class="n">tree</span><span class="p">.</span><span class="nf">getroot</span><span class="p">()</span>

<span class="c1"># Extract roads and junctions
</span><span class="n">roads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">junctions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">root</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">element</span><span class="p">.</span><span class="n">tag</span> <span class="o">==</span> <span class="sh">'</span><span class="s">road</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">roads</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">element</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">element</span><span class="p">.</span><span class="n">tag</span> <span class="o">==</span> <span class="sh">'</span><span class="s">junction</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">junctions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">element</span><span class="p">)</span>

<span class="c1"># Function to extract road details
</span><span class="k">def</span> <span class="nf">parse_road</span><span class="p">(</span><span class="n">road_element</span><span class="p">):</span>
    <span class="n">road_id</span> <span class="o">=</span> <span class="n">road_element</span><span class="p">.</span><span class="n">attrib</span><span class="p">[</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">road_length</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="n">road_element</span><span class="p">.</span><span class="n">attrib</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">length</span><span class="sh">'</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">{{</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="n">road_id</span><span class="p">,</span> <span class="sh">'</span><span class="s">length</span><span class="sh">'</span><span class="p">:</span> <span class="n">road_length</span><span class="p">}}</span>

<span class="n">parsed_roads</span> <span class="o">=</span> <span class="p">[</span><span class="nf">parse_road</span><span class="p">(</span><span class="n">road</span><span class="p">)</span> <span class="k">for</span> <span class="n">road</span> <span class="ow">in</span> <span class="n">roads</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">parse_junction</span><span class="p">(</span><span class="n">junction_element</span><span class="p">):</span>
    <span class="n">junction_id</span> <span class="o">=</span> <span class="n">junction_element</span><span class="p">.</span><span class="n">attrib</span><span class="p">[</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">connections</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">connection</span> <span class="ow">in</span> <span class="n">junction_element</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sh">'</span><span class="s">connection</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">incoming_road</span> <span class="o">=</span> <span class="n">connection</span><span class="p">.</span><span class="n">attrib</span><span class="p">[</span><span class="sh">'</span><span class="s">incomingRoad</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">connecting_road</span> <span class="o">=</span> <span class="n">connection</span><span class="p">.</span><span class="n">attrib</span><span class="p">[</span><span class="sh">'</span><span class="s">connectingRoad</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">connections</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
            <span class="sh">'</span><span class="s">incoming_road</span><span class="sh">'</span><span class="p">:</span> <span class="n">incoming_road</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">connecting_road</span><span class="sh">'</span><span class="p">:</span> <span class="n">connecting_road</span>
        <span class="p">})</span>
    <span class="k">return</span> <span class="p">{{</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">:</span> <span class="n">junction_id</span><span class="p">,</span> <span class="sh">'</span><span class="s">connections</span><span class="sh">'</span><span class="p">:</span> <span class="n">connections</span><span class="p">}}</span>

<span class="n">parsed_junctions</span> <span class="o">=</span> <span class="p">[</span><span class="nf">parse_junction</span><span class="p">(</span><span class="n">junction</span><span class="p">)</span> <span class="k">for</span> <span class="n">junction</span> <span class="ow">in</span> <span class="n">junctions</span><span class="p">]</span>
</code></pre></div></div> <p>From the OpenDRIVE map, we parsed 68 roads, with lengths such as:</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="w">
  </span><span class="p">{{</span><span class="w">
    </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"20"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"connections"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="p">{{</span><span class="w">
        </span><span class="nl">"incoming_road"</span><span class="p">:</span><span class="w"> </span><span class="s2">"13"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"connecting_road"</span><span class="p">:</span><span class="w"> </span><span class="s2">"21"</span><span class="w">
      </span><span class="p">}}</span><span class="w">
    </span><span class="p">]</span><span class="w">
  </span><span class="p">}},</span><span class="w">
  </span><span class="p">{{</span><span class="w">
    </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"55"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"connections"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="p">{{</span><span class="w">
        </span><span class="nl">"incoming_road"</span><span class="p">:</span><span class="w"> </span><span class="s2">"8"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"connecting_road"</span><span class="p">:</span><span class="w"> </span><span class="s2">"66"</span><span class="w">
      </span><span class="p">}}</span><span class="w">
    </span><span class="p">]</span><span class="w">
  </span><span class="p">}}</span><span class="w">
</span><span class="p">]</span><span class="w">
</span></code></pre></div></div> <h3 id="action-aware-route-generation">Action-aware Route Generation</h3> <p>Using the parsed roads and junctions, we generate routes between start and endpoints. Additionally, high-level driving commands such as <em>Left</em>, <em>Right</em>, and <em>Straight</em> are determined based on the relative orientation of consecutive road segments.</p> <p>The following code was used to generate random test cases:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span>

<span class="k">def</span> <span class="nf">generate_test_case</span><span class="p">(</span><span class="n">parsed_roads</span><span class="p">,</span> <span class="n">parsed_junctions</span><span class="p">):</span>
    <span class="n">start_road</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">parsed_roads</span><span class="p">)</span>
    <span class="n">end_road</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">parsed_roads</span><span class="p">)</span>

    <span class="n">route_length</span> <span class="o">=</span> <span class="n">start_road</span><span class="p">[</span><span class="sh">'</span><span class="s">length</span><span class="sh">'</span><span class="p">]</span> <span class="o">+</span> <span class="n">end_road</span><span class="p">[</span><span class="sh">'</span><span class="s">length</span><span class="sh">'</span><span class="p">]</span> <span class="o">+</span> <span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

    <span class="c1"># Please note here it's randomly generated
</span>    <span class="n">possible_commands</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Left</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Right</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Straight</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">commands</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">possible_commands</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>

    <span class="k">return</span> <span class="p">{{</span>
        <span class="sh">'</span><span class="s">start_index</span><span class="sh">'</span><span class="p">:</span> <span class="nf">int</span><span class="p">(</span><span class="n">start_road</span><span class="p">[</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">]),</span>
        <span class="sh">'</span><span class="s">end_index</span><span class="sh">'</span><span class="p">:</span> <span class="nf">int</span><span class="p">(</span><span class="n">end_road</span><span class="p">[</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">]),</span>
        <span class="sh">'</span><span class="s">route_length</span><span class="sh">'</span><span class="p">:</span> <span class="nf">round</span><span class="p">(</span><span class="n">route_length</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">commands</span><span class="sh">'</span><span class="p">:</span> <span class="n">commands</span>
    <span class="p">}}</span>

<span class="n">test_cases</span> <span class="o">=</span> <span class="p">[</span><span class="nf">generate_test_case</span><span class="p">(</span><span class="n">parsed_roads</span><span class="p">,</span> <span class="n">parsed_junctions</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
</code></pre></div></div> <table> <thead> <tr> <th><strong>Start Index</strong></th> <th><strong>End Index</strong></th> <th><strong>Route Length (m)</strong></th> <th><strong>High-Level Commands</strong></th> </tr> </thead> <tbody> <tr> <td>124</td> <td>70</td> <td>93.71</td> <td>Right, Straight</td> </tr> <tr> <td>110</td> <td>119</td> <td>89.68</td> <td>Straight, Straight</td> </tr> <tr> <td>288</td> <td>229</td> <td>113.14</td> <td>Straight, Left</td> </tr> <tr> <td>9</td> <td>4</td> <td>107.76</td> <td>Left, Straight</td> </tr> <tr> <td>111</td> <td>298</td> <td>89.86</td> <td>Straight, Right</td> </tr> </tbody> </table> <p>Below is the JSON representation of the generated test cases:</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="w">
    </span><span class="p">{{</span><span class="w">
        </span><span class="nl">"start_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">124</span><span class="p">,</span><span class="w">
        </span><span class="nl">"end_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">70</span><span class="p">,</span><span class="w">
        </span><span class="nl">"route_length"</span><span class="p">:</span><span class="w"> </span><span class="mf">93.71</span><span class="p">,</span><span class="w">
        </span><span class="nl">"commands"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"Right"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Straight"</span><span class="p">]</span><span class="w">
    </span><span class="p">}},</span><span class="w">
    </span><span class="p">{{</span><span class="w">
        </span><span class="nl">"start_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">110</span><span class="p">,</span><span class="w">
        </span><span class="nl">"end_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">119</span><span class="p">,</span><span class="w">
        </span><span class="nl">"route_length"</span><span class="p">:</span><span class="w"> </span><span class="mf">89.68</span><span class="p">,</span><span class="w">
        </span><span class="nl">"commands"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"Straight"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Straight"</span><span class="p">]</span><span class="w">
    </span><span class="p">}},</span><span class="w">
    </span><span class="p">{{</span><span class="w">
        </span><span class="nl">"start_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">288</span><span class="p">,</span><span class="w">
        </span><span class="nl">"end_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">229</span><span class="p">,</span><span class="w">
        </span><span class="nl">"route_length"</span><span class="p">:</span><span class="w"> </span><span class="mf">113.14</span><span class="p">,</span><span class="w">
        </span><span class="nl">"commands"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"Straight"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Left"</span><span class="p">]</span><span class="w">
    </span><span class="p">}},</span><span class="w">
    </span><span class="p">{{</span><span class="w">
        </span><span class="nl">"start_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">9</span><span class="p">,</span><span class="w">
        </span><span class="nl">"end_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
        </span><span class="nl">"route_length"</span><span class="p">:</span><span class="w"> </span><span class="mf">107.76</span><span class="p">,</span><span class="w">
        </span><span class="nl">"commands"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"Left"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Straight"</span><span class="p">]</span><span class="w">
    </span><span class="p">}},</span><span class="w">
    </span><span class="p">{{</span><span class="w">
        </span><span class="nl">"start_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">111</span><span class="p">,</span><span class="w">
        </span><span class="nl">"end_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">298</span><span class="p">,</span><span class="w">
        </span><span class="nl">"route_length"</span><span class="p">:</span><span class="w"> </span><span class="mf">89.86</span><span class="p">,</span><span class="w">
        </span><span class="nl">"commands"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"Straight"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Right"</span><span class="p">]</span><span class="w">
    </span><span class="p">}}</span><span class="w">
</span><span class="p">]</span><span class="w">
</span></code></pre></div></div> <h3 id="action-distance-aware-control">Action-Distance Aware Control</h3> <p>In our previous approach, the control system was designed to be <strong>action-aware</strong>. This means that between any two junctions (e.g., <code class="language-plaintext highlighter-rouge">Junction1</code> to <code class="language-plaintext highlighter-rouge">Junction2</code>), we defined the driving actions such as <em>Left</em>, <em>Right</em>, or <em>Straight</em>. However, this method does not take into account the distance required to perform these actions.</p> <p>This motivates us to redefine the control system as <strong>action-distance aware</strong>, where both the driving action and the associated distance are considered as part of the control strategy. By integrating distance awareness, we aim to control the precise execution of actions based on the distance travelled.</p> <p>In the new approach, each control instruction is associated with both an <strong>action</strong> and a <strong>distance</strong>. This means that for each route segment, we define not only the type of action to be taken but also the precise distance over which the action must be performed.</p> <h3 id="new-action-distance-aware-test-case">New Action-Distance Aware Test Case</h3> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{{</span><span class="w">
    </span><span class="nl">"start_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">124</span><span class="p">,</span><span class="w">
    </span><span class="nl">"end_index"</span><span class="p">:</span><span class="w"> </span><span class="mi">70</span><span class="p">,</span><span class="w">
    </span><span class="nl">"route_length"</span><span class="p">:</span><span class="w"> </span><span class="mf">93.71</span><span class="p">,</span><span class="w">
    </span><span class="nl">"commands"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">{{</span><span class="w">
            </span><span class="nl">"action"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Right"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"distance"</span><span class="p">:</span><span class="w"> </span><span class="mi">50</span><span class="w">
        </span><span class="p">}},</span><span class="w">
        </span><span class="p">{{</span><span class="w">
            </span><span class="nl">"action"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Straight"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"distance"</span><span class="p">:</span><span class="w"> </span><span class="mf">43.71</span><span class="w">
        </span><span class="p">}}</span><span class="w">
    </span><span class="p">]</span><span class="w">
</span><span class="p">}}</span><span class="w">
</span></code></pre></div></div> <p>In this new test case, the vehicle will:</p> <ul> <li>Execute a <em>Right</em> turn over the first 50 meters.</li> <li>Continue <em>Straight</em> for the remaining 43.71 meters.</li> </ul> <p>This new <strong>action-distance aware</strong> approach allows for a more flexible and accurate driving behavior, with both actions and distances specified for each route segment.</p> <p>We integrated distance measurement and action switching into model control. The video shows the model adjusting high-level commands based on distance, though some cases still need improvement.</p> <iframe width="700" height="500" src="https://www.youtube.com/embed/YmhcweTFlSU" title="Distance test case 1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[Parsing and generating driving test cases in CARLA Town02 map with OpenDRIVE]]></summary></entry><entry><title type="html">Coding week17 9/23-9/30</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week17/" rel="alternate" type="text/html" title="Coding week17 9/23-9/30"/><published>2024-09-30T00:00:00+00:00</published><updated>2024-09-30T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week17</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week17/"><![CDATA[<p>This week, I explored some works that integrate VQA with RL to enhance autonomous driving. This approach involves utilizing LLMs and transformer-based architectures to generate questions and responses based on driving scenarios.</p> <h3 id="vqa-in-autonomous-systems">VQA in Autonomous Systems</h3> <p>VQA leverages visual data to answer user-generated questions. In the context of autonomous driving, VQA can help process and analyze visual data, transforming real-time scenarios into structured interactions. For example, an autonomous vehicle using VQA might analyze traffic signals and generate questions regarding potential obstacles or safe navigation paths. VQA’s role in autonomous driving can be categorized into three core areas:</p> <p>For VQA in autonomous systems, core aspects include image understanding and object detection, with model accuracy in recognizing pedestrians, vehicles, and road signs. Extensive datasets enhance model capability for object differentiation.</p> <p>Additionally, contextual question generation offers support for decision-making, with relevant questions like “Is the pedestrian crossing clear?” at intersections aiding navigation. Real-time adaptability remains essential due to the dynamic nature of autonomous environments, which require high processing speeds and optimized algorithms to maintain safety and responsiveness without latency.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week17/framework-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week17/framework-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week17/framework-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/week17/framework.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I delved into this paper <d-cite key="atakishiyev2023explaining"></d-cite>. This architecture for autonomous driving uses a relatively straightforward structure, combining a DDPG-based RL agent with a VQA model. The use of pre-trained VGG-19 and fully connected layers for visual encoding suggests a simpler approach, likely prioritizing efficiency over complexity. The pipeline appears to rely heavily on offline processing of driving videos, where the pre-trained model and LSTM handle sequence-based vision and language tasks. While effective for basic visual and language alignment tasks, this setup may lack the sophistication needed for real-time adaptability or complex, dynamic driving scenarios due to its offline nature and relatively simple model architecture.</p> <h3 id="lingo-1-and-lingo-2">LINGO-1 and LINGO-2</h3> <p><strong>LINGO-1</strong> and <strong>LINGO-2</strong> <d-cite key="marcu2023lingoqa"></d-cite> provide questionnaire-based interactions within a controlled environment. These simulations enable the analysis of vehicle responses, situational understanding, and interactivity levels. Key features of these simulations include:</p> <ul> <li><strong>Questionnaire-Based Interactions:</strong> Simulations that integrate VQA allow for the generation of scenario-specific questions. This approach improves user engagement and adds layers of safety by ensuring the system’s responses align with the scenario.</li> <li><strong>Iterative Testing:</strong> By simulating various driving conditions, such as urban intersections or rural roads, researchers can test the adaptability of RL policies and refine VQA algorithms without risking real-world harm.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week17/lingo-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week17/lingo-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week17/lingo-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/week17/lingo.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="action-items">Action Items</h3> <p>Based on meeting insights, we will create a detailed technical plan, identifying open-source solutions for reproducibility testing.</p>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[This week, I explored some works that integrate VQA with RL to enhance autonomous driving. This approach involves utilizing LLMs and transformer-based architectures to generate questions and responses based on driving scenarios.]]></summary></entry><entry><title type="html">Coding week18 9/30-10/06</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week18/" rel="alternate" type="text/html" title="Coding week18 9/30-10/06"/><published>2024-09-30T00:00:00+00:00</published><updated>2024-09-30T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week18</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week18/"><![CDATA[<p>In recent discussions, we realize that the need for a distance-aware method to enhance the precision of autonomous driving actions within the CARLA simulator. Our previous control models lack a mechanism for dynamically adjusting actions based on the actual distance to objects or specific targets. By introducing a distance-aware approach, control actions can be fine-tuned by calculating the difference between initial and final positions. This capability is crucial for maneuvers like U-turns, where precise control over trajectory and stopping distances significantly impacts performance.</p> <p>In this week, LLMs are being explored for their potential to interpret natural language instructions, especially those that include ambiguous distance-related commands, such as “move a bit closer” or “stop at a safe distance.” By parsing these instructions, LLMs can generate actionable, distance-specific commands for the control module.</p> <p>In terms of validation, a proof-of-concept for the distance measurement within a simplified CARLA scenario was developed. This demo will test the accuracy by measuring how well the system calculates and responds to varying distances in real-time.</p> <h3 id="new-prompt-for-distance-instructions">New Prompt for Distance Instructions</h3> <p>In autonomous driving, instructions need to be clear, concise, and actionable. The new prompt system generates driving instructions by incorporating the specific action and distance parameters. Here’s how it works:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_instruction_prompt</span><span class="p">(</span><span class="n">action</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Generate a prompt for the OpenAI API to create a driving instruction for a given action.
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">
    You are generating driving instructions for an autonomous vehicle system. Each instruction should have a clear structure and follow these specific guidelines:

    1. The instruction should contain the following fields:
        - </span><span class="sh">'</span><span class="s">Instruction</span><span class="sh">'</span><span class="s">: A short, human-readable driving instruction that includes the action </span><span class="sh">'</span><span class="s">action</span><span class="sh">'</span><span class="s">. The instruction should also specify the distance or condition.
        - </span><span class="sh">'</span><span class="s">Action</span><span class="sh">'</span><span class="s">: This should be exactly one of the following: </span><span class="sh">'</span><span class="s">Right</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">Left</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">Straight</span><span class="sh">'</span><span class="s">, or </span><span class="sh">'</span><span class="s">LaneFollow</span><span class="sh">'</span><span class="s">. For this instruction, it should be </span><span class="sh">'</span><span class="s">action</span><span class="sh">'</span><span class="s">.
        - </span><span class="sh">'</span><span class="s">Distance</span><span class="sh">'</span><span class="s">: A distance measurement in meters (e.g., </span><span class="sh">'</span><span class="s">500 meters</span><span class="sh">'</span><span class="s">) or kilometers (e.g., </span><span class="sh">'</span><span class="s">1.2 kilometers</span><span class="sh">'</span><span class="s">). This should reflect how far the action applies or when the next instruction should be executed.

    2. The </span><span class="sh">'</span><span class="s">Instruction</span><span class="sh">'</span><span class="s"> should be realistic and concise, similar to how a navigation system provides directions. Here are three detailed examples:
        - Example 1:
          - Instruction: </span><span class="sh">"</span><span class="s">Turn right at the next intersection and continue for 500 meters.</span><span class="sh">"</span><span class="s">
          - Action: </span><span class="sh">"</span><span class="s">Right</span><span class="sh">"</span><span class="s">
          - Distance: </span><span class="sh">"</span><span class="s">500 meters</span><span class="sh">"</span><span class="s">
        - Example 2:
          - Instruction: </span><span class="sh">"</span><span class="s">Proceed straight for 1.5 kilometers until you reach the gas station.</span><span class="sh">"</span><span class="s">
          - Action: </span><span class="sh">"</span><span class="s">Straight</span><span class="sh">"</span><span class="s">
          - Distance: </span><span class="sh">"</span><span class="s">1.5 kilometers</span><span class="sh">"</span><span class="s">
        - Example 3:
          - Instruction: </span><span class="sh">"</span><span class="s">Follow the lane for 3 kilometers and exit at the next junction.</span><span class="sh">"</span><span class="s">
          - Action: </span><span class="sh">"</span><span class="s">LaneFollow</span><span class="sh">"</span><span class="s">
          - Distance: </span><span class="sh">"</span><span class="s">3 kilometers</span><span class="sh">"</span><span class="s">

    3. Ensure the action </span><span class="sh">'</span><span class="s">action</span><span class="sh">'</span><span class="s"> is embedded naturally in the </span><span class="sh">'</span><span class="s">Instruction</span><span class="sh">'</span><span class="s">, and that it specifies when or where the action should occur.

    4. The generated JSON output should have the following structure:
    Instruction

    Based on this structure, generate a driving instruction for the action </span><span class="sh">'</span><span class="s">action</span><span class="sh">'</span><span class="s">.
    </span><span class="sh">"""</span>

</code></pre></div></div> <p>For example, an output could be:</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"Instruction"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Follow the lane for 2 kilometers until you reach the gas station."</span><span class="p">,</span><span class="w">
    </span><span class="nl">"Action"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LaneFollow"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"Distance"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2 kilometers"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>We validated smooth action transitions in Town01 using PID control for proof of concept. As shown in the following video.</p> <iframe width="700" height="500" src="https://www.youtube.com/embed/GvBChwSAYfk" title="U-turn Town 01" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <h3 id="distance-measurement">Distance Measurement</h3> <p>The system includes a function to measure the distance traveled, which is essential for determining when to transition from one action to another. The function uses the Euclidean distance formula to calculate the distance between two points:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">find_dist</span><span class="p">(</span><span class="n">loc1</span><span class="p">,</span> <span class="n">loc2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">((</span><span class="n">loc2</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">loc1</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">loc2</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">loc1</span><span class="p">.</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div> <ul> <li><strong>Explanation</strong>: The <code class="language-plaintext highlighter-rouge">find_dist</code> function computes the distance in CARLA’s 2D plane (X, Y) between the starting position (<code class="language-plaintext highlighter-rouge">loc1</code>) and the vehicle’s current position (<code class="language-plaintext highlighter-rouge">loc2</code>).</li> <li><strong>Usage</strong>: The system calculates the <code class="language-plaintext highlighter-rouge">dist_travelled</code> by comparing the start and current locations, adjusting the vehicle’s controls based on whether the required distance has been reached.</li> </ul> <p>Example application:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">start_location</span> <span class="o">=</span> <span class="n">vehicle</span><span class="p">.</span><span class="nf">get_location</span><span class="p">()</span>
<span class="n">current_location</span> <span class="o">=</span> <span class="n">vehicle</span><span class="p">.</span><span class="nf">get_location</span><span class="p">()</span>
<span class="n">dist_travelled</span> <span class="o">=</span> <span class="nf">find_dist</span><span class="p">(</span><span class="n">start_location</span><span class="p">,</span> <span class="n">current_location</span><span class="p">)</span> <span class="c1"># Calculate the distance traveled
</span></code></pre></div></div> <p>In the <code class="language-plaintext highlighter-rouge">drive_forward</code> function, this logic ensures the vehicle halts upon reaching the specified distance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">dist_travelled</span> <span class="o">&gt;=</span> <span class="n">distance</span><span class="p">:</span>
    <span class="n">control</span> <span class="o">=</span> <span class="n">carla</span><span class="p">.</span><span class="nc">VehicleControl</span><span class="p">(</span><span class="n">throttle</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">brake</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># Stop the vehicle
</span>    <span class="n">vehicle</span><span class="p">.</span><span class="nf">apply_control</span><span class="p">(</span><span class="n">control</span><span class="p">)</span>
    <span class="k">break</span>
</code></pre></div></div> <p>In the Town01 map, we tested whether the real-time distance measurement module was functioning correctly. A video demonstration is provided below.</p> <iframe width="700" height="500" src="https://www.youtube.com/embed/5Goq_FYiyeU" title="Distance Measurement 1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[In recent discussions, we realize that the need for a distance-aware method to enhance the precision of autonomous driving actions within the CARLA simulator. Our previous control models lack a mechanism for dynamically adjusting actions based on the actual distance to objects or specific targets. By introducing a distance-aware approach, control actions can be fine-tuned by calculating the difference between initial and final positions. This capability is crucial for maneuvers like U-turns, where precise control over trajectory and stopping distances significantly impacts performance.]]></summary></entry><entry><title type="html">Coding week15&amp;amp;16 9/09-9/22</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week1516/" rel="alternate" type="text/html" title="Coding week15&amp;amp;16 9/09-9/22"/><published>2024-09-22T00:00:00+00:00</published><updated>2024-09-22T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week1516</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week1516/"><![CDATA[<p>This week, we focused on exploring new ideas for enhancing autonomous vehicle capabilities with LLMs language instructions. Below is a summary of the primary ideas we considered, aiming to address current limitations and expand on innovative interaction methods between passengers and self-driving systems.</p> <p>This week, we focused on exploring new ideas for enhancing autonomous vehicle capabilities with LLMs language instructions.</p> <ul> <li> <p>Idea Selection and Planning: Sharing the complete list of ideas from this meeting and subsequent discussions will allow us to identify and finalize one or two core directions.</p> </li> <li> <p>Feasibility Research: We will also conduct feasibility studies on the selected ideas to explore technical challenges and identifying necessary tools, frameworks, and methodologies. This research will be crucial in ensuring that the chosen solutions are practically implementable for future development.</p> </li> </ul> <p>Below is a summary of the primary ideas explored.</p> <h2 id="driving-with-natural-language">Driving with Natural Language</h2> <h3 id="talk2car-taking-control-of-your-self-driving-car-">Talk2Car: Taking Control of Your Self-Driving Car <d-cite key="deruyttere_2019_Talk2CarTakingControl"></d-cite></h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week1516/talk2car-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week1516/talk2car-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week1516/talk2car-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/week1516/talk2car.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We began by exploring potential limitations from the early stages of the work.</p> <ul> <li> <p><strong>Motivation</strong>: Autonomous vehicles may hesitate in some complex traffic scenarios, especially when roads are congested or special situations arise. If passengers can provide suggestions or instructions through natural language commands, such as asking the vehicle to stop or wait, it can help the vehicle make quicker decisions.</p> </li> <li> <p><strong>Limited to Object Reference Tasks</strong>: The core issue of the article is to map the passenger’s natural language commands to specific objects in the visual scene. This “object reference” task mainly aims to enable autonomous vehicles to identify specific objects mentioned in passenger instructions, without involving direct vehicle control or executing route planning. Therefore, the article focuses solely on “recognizing reference objects”.</p> </li> <li> <p><strong>Not Involve Real Control</strong>: Although the article discusses the motivation for allowing passengers to interact with autonomous vehicles through natural language, the current research is limited to recognizing objects indicated by passengers through natural language <strong>recognition</strong> and not actually executing these instructions (e.g., stopping, turning, etc.).</p> </li> </ul> <h3 id="conditional-driving-from-natural-language-instructions-">Conditional Driving from Natural Language Instructions <d-cite key="roh20a"></d-cite></h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week1516/conditional_driving-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week1516/conditional_driving-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week1516/conditional_driving-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/week1516/conditional_driving.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Previous work primarily focus on path planning and navigation systems. However, as human-machine interaction evolves, we realize that human expectations for autonomous driving systems extend beyond simple navigation commands to a more “coaching” role in interactions. In such interactions, autonomous driving systems need not only basic path planning capabilities but also the ability to understand complex, multi-turn instructions, demonstrate keen environmental insights, and effectively handle edge or hazardous scenarios. This motivation from this paper has driven the proposal and exploration of the following research directions.</p> <h3 id="talk-to-the-vehicle-language-conditioned-autonomous-navigation-of-self-driving-cars-">Talk to the Vehicle: Language-Conditioned Autonomous Navigation of Self-Driving Cars <d-cite key="sriram_2019_TalkVehicleLanguageb"></d-cite></h3> <p>The motivation for the article is based on the idea that in many complex road scenarios, traditional autonomous driving technology relies on pre-generated detailed maps and precise positioning systems. When these conditions are not met, such as when maps are inaccurate or GPS delays occur, vehicles may fail to navigate correctly.</p> <p>Human navigation relies not on precise map data but on the <strong>semantic understanding of language and the current environment.</strong> By adopting this method, autonomous vehicles can navigate through semantic understanding and natural language instructions without relying on detailed offline maps.</p> <p>By introducing natural language instructions, vehicles can complete navigation tasks without accurate maps or positioning information. This reduces the dependence on external maps and GPS accuracy. This would make autonomous driving technology more robust and adaptable, especially in uncontrollable outdoor environments.</p> <p>Therefore, the research motivation is to explore how to enhance the navigation capabilities of autonomous vehicles through natural language instructions. This approach can not only improve the efficiency of autonomous driving but also allow autonomous vehicles to integrate more naturally into human driving behaviors, especially in the face of GPS positioning errors or the absence of pre-mapped maps.</p> <p>The primary inspiration for this paper is that, <strong>in real life, human drivers can easily complete navigation tasks with simple language instructions (such as “turn right at the traffic light, then turn left at the second intersection”) even without precise maps.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week1516/framework_talk2vehicle-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week1516/framework_talk2vehicle-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week1516/framework_talk2vehicle-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/week1516/framework_talk2vehicle.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The framework proposed in the article has three core modules:</p> <ul> <li>NLE: Transforms natural language instructions into high-level machine-readable encodings.</li> <li>Waypoint Generation Network (WGN): Combines local semantic structure with language encoding to predict local waypoints.</li> <li>Generates obstacle-avoidance trajectories based on predicted waypoints, executed by the low-level controller.</li> <li>By combining natural language with visual and semantic maps, vehicles can generate waypoints adapted to the current environment, and this method avoids dependence on detailed maps and precise positioning. Each time local waypoints are generated, WGN considers the language instructions and the vehicle’s current environmental information.</li> </ul> <h2 id="long-turn-interaction">Long-Turn Interaction</h2> <h3 id="drivlme-enhancing-llm-based-autonomous-driving-agents-with-embodied-and-social-experiences-">DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences <d-cite key="huang__DriVLMeEnhancingLLMbased"></d-cite></h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week1516/talk2vehicle-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week1516/talk2vehicle-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week1516/talk2vehicle-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/week1516/talk2vehicle.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The motivation of the article is to address the limitations of existing autonomous driving technologies in actual complex driving scenarios, especially the problems of <strong>long-duration navigation tasks</strong> and <strong>free dialogue interactions</strong>. Although current foundational models (FMs) have shown the potential to handle short-term tasks, they still face challenges when dealing with environmental dynamics, task changes, or long-term human-vehicle interactions:</p> <p>Existing autonomous driving systems are proficient at executing simple, short-term tasks like turning or overtaking but fall short when it comes to understanding broader, goal-oriented tasks that involve route planning and map knowledge. Additionally, these systems struggle to handle unexpected situations that arise from sensor limitations, environmental changes, or shifts in task requirements. Traditional systems are also limited in managing natural language dialogue, making it difficult for them to engage in complex, multi-turn interactions with passengers, especially in dynamic and evolving environments where continuous context understanding and appropriate responses are essential.</p> <h3 id="dorothie-spoken-dialogue-for-handling-unexpected-situations-in-interactive-autonomous-driving-agents-">DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents <d-cite key="ma_2022_DOROTHIESpokenDialogue"></d-cite></h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week1516/multi_interaction-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week1516/multi_interaction-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week1516/multi_interaction-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/week1516/multi_interaction.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Motivation</strong>: User interaction with autonomous driving systems is often multi-turn, involving complex, long-duration instruction processing. To achieve this, the system must have the capability to handle long-term instructions, such as mapless navigation (Mapless Navigation) and following complex instructions (Multi-turn Interactions, Long Horizon Instructions). Traditional navigation systems only need to provide a path and execute it, but the new interaction mode requires the system to flexibly adapt to new instructions, even dynamically adjusting paths and goals during the task process.</p> <h3 id="potential-research-directions">Potential Research Directions</h3> <ol> <li> <p>Handling Ambiguity in Language Instructions: Language often contains vague or uncertain expressions, like “go forward a bit, then turn right.” Without adequate context-understanding, such vague instructions may lead to navigation errors.</p> <ul> <li>Potential research direction: Employing advanced language models (e.g., GPT series) that are better equipped to interpret ambiguous language, handle diverse expressions, and reason based on context.</li> </ul> </li> <li> <p>Semantic Understanding of Environment:</p> <p>The current system uses a waypoint generation network that combines semantic information and language encoding to produce local waypoints. However, this integration is still “static.”</p> <ul> <li>Challenges: <ul> <li>The system assumes that combining language encoding with the current semantic map suffices, but real-world scenarios often involve mismatches between environmental perception and language instructions. For instance, there could be multiple “left turns,” making it unclear which one to follow.</li> <li>While LLMs can handle complex multimodal data, the current system lacks the deeper interaction and reasoning needed to optimally use environmental and language information together.</li> </ul> </li> </ul> </li> <li> <p>Lack of Long-Term Understanding:</p> <p>Although the system’s local planning approach offers flexibility, it lacks a global perspective. Only planning local waypoints can lead to that local planning may not always find the most efficient path, potentially leading to detours or longer routes.</p> </li> <li> <p>Inadequate Exception Handling Capabilities:</p> <p>The system’s reliance on real-time sensor input and waypoint updates can make it vulnerable to unexpected situations if sensor data fails or feedback is inaccurate.</p> <ul> <li>Challenges: <ul> <li>The current system might fail to respond effectively to emergencies, such as unexpected obstacles or environmental changes.</li> <li>Although LLMs provide robust reasoning and language processing, integrating these models with dynamic environmental feedback remains limited.</li> </ul> </li> <li>Potential research direction: Investigating RL or feedback mechanisms based on historical data to help the system adjust autonomously during emergencies and dynamically optimize its path. Additionally, integrating LLMs with sensor feedback could enhance response to complex conditions.</li> </ul> </li> </ol> <h3 id="advanced-frameworks">Advanced Frameworks</h3> <ol> <li> <p>World Model Based on Real Data Research increasingly focuses on using real data to train systems for better environmental cognition, enabling systems not only to process immediate perceptual data but also to predict and anticipate complex traffic situations. This model aids in handling variable traffic scenarios, especially in dynamically changing or uncertain environments <d-cite key="guan_2024_WorldModelsAutonomous"></d-cite>.</p> </li> <li> <p>Language Generation and Embodied Experiences Language generation should go beyond preset corpora by incorporating embodied experiences. Autonomous driving systems can benefit from extracting and interpreting information from the environment and translating it into natural language feedback. In complex traffic situations, the system could assess road conditions in real-time and communicate this to users.</p> </li> </ol>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[This week, we focused on exploring new ideas for enhancing autonomous vehicle capabilities with LLMs language instructions. Below is a summary of the primary ideas we considered, aiming to address current limitations and expand on innovative interaction methods between passengers and self-driving systems.]]></summary></entry><entry><title type="html">Coding week14 8/26-9/08</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week14/" rel="alternate" type="text/html" title="Coding week14 8/26-9/08"/><published>2024-09-08T00:00:00+00:00</published><updated>2024-09-08T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week14</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week14/"><![CDATA[<p>This week, we focused on exploring several potential research to create an “Ideas List”, which is a collection of possible research concepts. As we evaluate each idea in alignment with project goals, our approach includes balancing technical feasibility with anticipated resource needs and practical challenges.</p> <p>This list includes ideas focused on recent advancements, integrations with LLMs, and potential research gaps with current works. During our review, we evaluated each idea based on several key factors to identify those that promise impact while remaining feasible given our technical and resource constraints. This post outlines the main points from our discussions, along with actionable insights and next steps.</p> <h3 id="update">Update</h3> <p>In addition to our research review and ideas list development, the following updates were made:</p> <ul> <li> <p>PRs Merged into Main Branch: Two pull requests, <a href="https://github.com/TheRoboticsClub/gsoc2024-ZebinHuang/commit/39659853cd7e4aab5a2303b6faa62d9b62cbe4ff">PR#3</a> and <a href="https://github.com/TheRoboticsClub/gsoc2024-ZebinHuang/commit/53bf78de6a39754cbae95d118385162c82aa3d54">PR#5</a>, were successfully merged into the main branch. PR#3 focused on the model with the CARLA simulation for testing and validation, while PR#5 addressed web-based streamlit app, packaged for online deployment using the Streamlit framework.</p> </li> <li> <p>Social Media Posts: Posts were published on LinkedIn and Twitter featuring the latest video demonstration. （See <a href="https://www.linkedin.com/feed/update/urn:li:activity:7240524535718952960">LinkedIn post</a>)</p> </li> <li> <p>Documentation Update: The app documentation was revised and updated to clearly distinguish between the development version and the deployable web version.</p> </li> </ul> <iframe width="700" height="500" src="https://www.youtube.com/embed/8RdJSK0M_uc" title="GSoC24 Midterm Demo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <h3 id="resource-availability">Resource Availability</h3> <p>We examined the viability of implementing from both technical and resource-based perspectives. This included considering our current toolset and any additional resources that may be required to bring an idea to fruition. Access to high-performance computing resources has emerged as a critical consideration, as the computational demands of the advance LLMs projects currently exceed our available resources. Given this, we are actively exploring options like using gaming GPUs, such as the RTX 4060.</p> <p>In autonomous driving model design, different architectures significantly impact GPU resource utilization. In this discussion, we use SparseDrive and LMDrive as examples to see these trade-offs in GPU resource utilization.</p> <p>The SparseDrive model achieves computational efficiency through a sparse representation framework, which minimizes reliance on dense bird’s-eye view (BEV) features, thereby reducing resource consumption, particularly in multi-GPU setups <d-cite key="sun2024sparsedrive"></d-cite>. Specifically, SparseDrive employs ResNet50 and ResNet101 backbones and train through a parallelized approach for perception and planning tasks. On an 8x NVIDIA RTX 4090 GPU system, SparseDrive demonstrates up to sevenfold increases in training and inference speeds compared to models such as UniAD <d-cite key="hu2023planning"></d-cite>, which traditionally employ dense representations. This efficiency is due to SparseDrive’s reduced floating-point operations (FLOP) requirements and decreased memory usage in its sparse, hierarchical planning structure, resulting in enhanced scalability and throughput with fewer GPU requirements <d-cite key="sun2024sparsedrive"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week14/sparsedrive-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week14/sparsedrive-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week14/sparsedrive-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/week14/sparsedrive.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In contrast, LMDrive’s <d-cite key="shao2024lmdrive"></d-cite> architecture is more resource-intensive for closed-loop, language-guided autonomous driving. LMDrive incorporates multimodal encoders and additional adapters, such as Q-Formers and token adapters, to handle both visual and textual data inputs. This design supports the processing of extensive multi-view camera and LiDAR data and consequently increasing computational requirements relative to SparseDrive. The LLaMA-based language backbone also requires memory and processing power.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/week14/lmdrive-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/week14/lmdrive-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/week14/lmdrive-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/week14/lmdrive.png" class="img-fluid rounded z-depth-1 w-60 mx-auto d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Training LMDrive requires approximately 4-6 days on 8 A100 GPUs with 80GB memory and consists of two stages: vision encoder pre-training and instruction fine-tuning, as outlined in their <a href="https://github.com/opendilab/LMDrive/blob/main/README.md#training">documentation</a>. LMDrive’s large parameter count, coupled with the need for real-time closed-loop processing, imposes a substantial load on GPU memory; however, it achieves robustness in language-guided navigation and adaptive control.</p> <h3 id="action-items">Action Items</h3> <p>To carry forward the selected ideas, we outlined specific action items. These steps are critical to ensuring that our top-priority ideas move steadily through the development pipeline. The key tasks include:</p> <ul> <li><strong>Idea Selection and Planning:</strong> Publishing the complete ideas list from this meeting and in further discussions will allow us to finalize one or two core ideas that the can focus on developing.</li> <li><strong>Feasibility Research:</strong> Comprehensive research into the technical feasibility of the selected ideas will enable us to identify specific tools, frameworks, and methodologies required for future successful implementation.</li> </ul>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[This week, we focused on exploring several potential research to create an “Ideas List”, which is a collection of possible research concepts. As we evaluate each idea in alignment with project goals, our approach includes balancing technical feasibility with anticipated resource needs and practical challenges.]]></summary></entry><entry><title type="html">Coding week12&amp;amp;13 8/12-8/25</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week1213/" rel="alternate" type="text/html" title="Coding week12&amp;amp;13 8/12-8/25"/><published>2024-08-25T00:00:00+00:00</published><updated>2024-08-25T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week1213</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week1213/"><![CDATA[<p>In recent weeks, we have focused on enhancing model evaluation, deployment workflows, video presentation quality, and looking for next research objectives. The August 14 and 21 meetings highlighted our progress and set new goals, the meeting details are summarized in <a href="https://docs.google.com/document/d/1b2ZEU5Gt8gP2ae_YzNSJSd7RukUrsG_aDJFLnbvoQiM/edit">Google Doc</a>.</p> <h3 id="model-evaluation-challenges">Model Evaluation Challenges</h3> <p>In the August 14 meeting, regarding the research idea, we discussed the model’s performance in handling varied scenarios, especially focusing on collision avoidance mechanisms. Achieving high predictive accuracy remains a significant challenge, as it is essential for enhancing the model’s reliability across various simulated environments. Currently, our primary focus is on improving model accuracy through expanding the diversity of training user instruction data and scenario configurations.</p> <h3 id="model-deployment">Model Deployment</h3> <p>To address GitHub storage limitations and Hugging Face compatibility, we proposed streamlined workflows for easier updates. Key improvements include integrating OpenAI API support to simplify model loading. Next steps are to complete testing and documentation for the OpenAI API. <a href="https://github.com/TheRoboticsClub/gsoc2024-ZebinHuang/pull/5">See PR here</a></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/gsoc2024-ZebinHuang/assets/img/api_key-480.webp 480w,/gsoc2024-ZebinHuang/assets/img/api_key-800.webp 800w,/gsoc2024-ZebinHuang/assets/img/api_key-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/gsoc2024-ZebinHuang/assets/img/api_key.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="action-items">Action Items</h3> <p>Based on current works, the following tasks were outlined as priorities:</p> <ul> <li><strong>Immediate Fixes:</strong> Address issues in an open Pull Request on the CARLA branch, resolve Streamlit errors, integrate OpenAI API, and fix model loading challenges.</li> <li><strong>Research and Documentation:</strong> Conduct a literature review and document findings for research continuity.</li> <li><strong>Social Media Engagement:</strong> After reviewing the 1-minute demo video, it was agreed that the captions required adjustment for clarity, and background music will be added to increase appeal. Once these enhancements are complete, I will draft a LinkedIn post summarizing project achievements and outcomes to accompany the video. Draft a LinkedIn post to accompany the 1-minute demo to show recent outcomes.</li> </ul>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[In recent weeks, we have focused on enhancing model evaluation, deployment workflows, video presentation quality, and looking for next research objectives. The August 14 and 21 meetings highlighted our progress and set new goals, the meeting details are summarized in Google Doc.]]></summary></entry><entry><title type="html">Coding week10&amp;amp;11 7/29-8/11</title><link href="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/coding-week8-7-29-8-11/" rel="alternate" type="text/html" title="Coding week10&amp;amp;11 7/29-8/11"/><published>2024-08-21T00:00:00+00:00</published><updated>2024-08-21T00:00:00+00:00</updated><id>https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/week1011</id><content type="html" xml:base="https://theroboticsclub.github.io/gsoc2024-ZebinHuang/blog/2024/coding-week8-7-29-8-11/"><![CDATA[<p>Over the past two weeks, our focus has been on addressing several tasks for the ongoing development of our project. Here is a summary of the key action items, progress, and additional insights that have emerged during this period:</p> <ol> <li><strong>Mid-Term Evaluation</strong>: <ul> <li>We have been meticulously preparing for the mid-term evaluation, including our progress, documentation, and demos. The feedback from this evaluation will guide the next steps of our development.</li> </ul> </li> <li><strong>Fixing Video Issues in the Blog</strong>: <ul> <li>To resolve this, the video will be uploaded to a trusted platform like YouTube or Google Drive, ensuring that it is easily accessible to all viewers.</li> </ul> </li> <li><strong>Mid-Term Demo Video</strong>: <ul> <li>A critical component of our mid-term deliverables is the demonstration video showcasing the current capabilities of our model within the simulation environment. This video will highlight how the model processes and classifies different driving scenarios.</li> </ul> </li> <li><strong>Improving Dataset Evaluation</strong>: <ul> <li>We are actively exploring ways to enhance the synthetic dataset generation process, focusing on creating more diverse datasets without directly embedding commands into the instructions. The goal is to ensure that the generated data is both rich in context and relevant to our classification tasks. We are using resources such as: <ul> <li><a href="https://huggingface.co/learn/cookbook/rag_evaluation">RAG Evaluation Cookbook</a></li> <li><a href="https://huggingface.co/learn/cookbook/llm_judge">LLM Judge Evaluation Cookbook</a></li> </ul> </li> <li>These resources provide some inspiration for improving the evaluation process.</li> </ul> </li> <li><strong>Future Works</strong>: <ul> <li>As we look ahead, we are conducting a literature review to identify potential research avenues that can further enhance our project. The LMDrive repository and similar projects offer valuable insights into how we can refine our approach and explore new research ideas. We are particularly interested in extending our work to incorporate more advanced LLM techniques.</li> <li>Additionally, to show the project in action, all scripts are integrated into a web app through the Streamlit platform.</li> </ul> </li> </ol> <h3 id="streamlit-development">Streamlit Development</h3> <p>The recent development efforts have been centred on implementing a Streamlit-based app for our project. This app can make the tool more accessible and user-friendly.</p> <p><strong>Design and Architecture</strong></p> <ul> <li>The Streamlit app has been designed with a modular architecture, allowing for easy scaling and adaptation. I have already encapsulated the scripts independently, which now allows for quick splitting and building of the app through different modular pages.</li> <li>The main pages include: <ul> <li><strong>Data Generation</strong>: Users can generate datasets required for training models. This page provides the tools necessary to create diverse and robust datasets.</li> <li><strong>Data Analysis</strong>: This section allows users to analyze the generated or uploaded data. Visualization tools are integrated to help users understand the data distribution and key metrics at a glance.</li> <li><strong>Model Training</strong>: In this section, users can initiate model training sessions. The interface includes options for viewing logs and evaluating interim results. It also allows for customization of training parameters to optimize performance.</li> <li><strong>Check Logs</strong>: Users can review detailed logs from the data generation, analysis, and model training processes. This helps in debugging and ensures transparency in the operations performed by the app.</li> <li><strong>Model Testing</strong>: This page allows users to test the BERT model online with single instructions or files.</li> </ul> </li> </ul> <p><strong>Data Import/Export Interface</strong></p> <ul> <li>For online deployment, we should also implement a data import/export mechanism. Unlike the local environment, the online version requires handling real-time data flow effectively. We implemented a streamlined interface that allows users to upload data files easily and download results in various formats.</li> <li>Below is a snapshot of the code handling data import/export:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_pdf_report</span><span class="p">(</span><span class="n">train_fig</span><span class="p">,</span> <span class="n">eval_fig</span><span class="p">,</span> <span class="n">train_log</span><span class="p">,</span> <span class="n">eval_log</span><span class="p">,</span> <span class="n">cls_report</span><span class="p">,</span> <span class="n">folder_path</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Convert logs and figures into an HTML and generate a PDF report.</span><span class="sh">"""</span>

    <span class="c1"># Convert train figure to base64
</span>    <span class="n">buffer1</span> <span class="o">=</span> <span class="n">io</span><span class="p">.</span><span class="nc">BytesIO</span><span class="p">()</span>
    <span class="n">train_fig</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">buffer1</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="sh">'</span><span class="s">png</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">buffer1</span><span class="p">.</span><span class="nf">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">train_base64</span> <span class="o">=</span> <span class="n">base64</span><span class="p">.</span><span class="nf">b64encode</span><span class="p">(</span><span class="n">buffer1</span><span class="p">.</span><span class="nf">read</span><span class="p">()).</span><span class="nf">decode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">train_img_html</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="sh">'</span><span class="s">&lt;img src=</span><span class="sh">"</span><span class="s">data:image/png;base64,</span><span class="si">{</span><span class="n">train_base64</span><span class="si">}</span><span class="sh">"</span><span class="s"> </span><span class="sh">'</span>
        <span class="sh">'</span><span class="s">style=</span><span class="sh">"</span><span class="s">width: 80%; max-width: 800px;</span><span class="sh">"</span><span class="s">/&gt;</span><span class="sh">'</span>
    <span class="p">)</span>

    <span class="c1"># Convert eval figure to base64
</span>    <span class="n">buffer2</span> <span class="o">=</span> <span class="n">io</span><span class="p">.</span><span class="nc">BytesIO</span><span class="p">()</span>
    <span class="n">eval_fig</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="n">buffer2</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="sh">'</span><span class="s">png</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">buffer2</span><span class="p">.</span><span class="nf">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">eval_base64</span> <span class="o">=</span> <span class="n">base64</span><span class="p">.</span><span class="nf">b64encode</span><span class="p">(</span><span class="n">buffer2</span><span class="p">.</span><span class="nf">read</span><span class="p">()).</span><span class="nf">decode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">eval_img_html</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="sh">'</span><span class="s">&lt;img src=</span><span class="sh">"</span><span class="s">data:image/png;base64,</span><span class="si">{</span><span class="n">eval_base64</span><span class="si">}</span><span class="sh">"</span><span class="s"> </span><span class="sh">'</span>
        <span class="sh">'</span><span class="s">style=</span><span class="sh">"</span><span class="s">width: 80%; max-width: 800px;</span><span class="sh">"</span><span class="s">/&gt;</span><span class="sh">'</span>
    <span class="p">)</span>

    <span class="c1"># Generate markdown for logs
</span>    <span class="n">train_log_md</span><span class="p">,</span> <span class="n">eval_log_md</span><span class="p">,</span> <span class="n">cls_report_md</span> <span class="o">=</span> <span class="nf">generate_markdown</span><span class="p">(</span>
        <span class="n">train_log</span><span class="p">,</span> <span class="n">eval_log</span><span class="p">,</span> <span class="n">cls_report</span>
    <span class="p">)</span>
    <span class="n">train_html</span><span class="p">,</span> <span class="n">eval_html</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nf">markdown</span><span class="p">(</span><span class="n">train_log_md</span><span class="p">),</span>
        <span class="nf">markdown</span><span class="p">(</span><span class="n">eval_log_md</span><span class="p">),</span>
        <span class="nf">markdown</span><span class="p">(</span><span class="n">cls_report_md</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="c1"># Create HTML content
</span>    <span class="n">html_content</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">
    &lt;!DOCTYPE html&gt;
    &lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;Markdown&lt;/title&gt;
        &lt;style&gt;
            body 
            img 
            .page-break 
        &lt;/style&gt;
    &lt;/head&gt;
    &lt;body&gt;
        </span><span class="si">{</span><span class="n">train_html</span><span class="si">}</span><span class="s">
        </span><span class="si">{</span><span class="n">train_img_html</span><span class="si">}</span><span class="s">
        </span><span class="si">{</span><span class="n">eval_html</span><span class="si">}</span><span class="s">
        </span><span class="si">{</span><span class="n">eval_img_html</span><span class="si">}</span><span class="s">

    &lt;/body&gt;
    &lt;/html&gt;
    </span><span class="sh">"""</span>

    <span class="c1"># Convert HTML to PDF
</span>    <span class="n">pdf_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">folder_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">logs_report.pdf</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">convert_html_to_pdf</span><span class="p">(</span><span class="n">html_content</span><span class="p">,</span> <span class="n">pdf_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pdf_path</span>
</code></pre></div></div> <p><strong>Challenges in Model Storage</strong></p> <ul> <li>A significant technical challenge was how to manage the storage and export of models trained online. Given the constraints of bandwidth and storage, exporting large models like BERT is not feasible within our current setup. Therefore, we opted for TinyBERT, a more compact model that has shown excellent performance in our tests, achieving nearly 100% accuracy in the scenarios we’ve evaluated.</li> <li>Referring to this article <a href="https://blog.streamlit.io/common-app-problems-resource-limits/">here</a>, Streamlit currently has resource limitations, which prevent us from freely training and exporting pre-trained models. This feature in our app is not yet fully supported. We are considering using Github’s Large File Storage, but this solution has not been fully tested yet.</li> </ul> <h3 id="llm-evaluation"><strong>LLM Evaluation</strong></h3> <p>In addition to implementing the Streamlit app, we have been exploring various cookbooks from Hugging Face suggested by mentors to refine our approach. However, this exploration has also revealed several challenges:</p> <ol> <li><strong>Sample Size Insufficiency</strong>: The current dataset might not be large enough to fully train more complex models. We are considering various data augmentation strategies to increase the dataset’s size and diversity.</li> <li><strong>Evaluation of Human Input</strong>: The evaluation methods we currently use are heavily reliant on human judgment. To address this, we are exploring automated evaluation techniques that can provide consistent and scalable assessments.</li> <li><strong>Lack of Trials</strong>: The limited availability of real-world data and trials has been a bottleneck. To overcome this, some self-supervised methods should be explored.</li> </ol> <h3 id="model-inference">Model Inference</h3> <p>We conducted a series of comparative experiments to determine whether the model inference stage significantly impacts decision-making within the simulation. Our findings indicate that the model’s decisions remain consistent, even under varied conditions.</p> <p>Interestingly, we discovered that some collision issues initially thought to be caused by the model inference stage, were actually rooted in problems with the previous model version. To provide more insight, we have attached both error videos and examples of correct simulations.</p> <iframe width="700" height="500" src="https://www.youtube.com/embed/1Fg6R5mZLHE" title="Control with bert" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <iframe width="700" height="500" src="https://www.youtube.com/embed/urk-g_el_gg" title="Crash without bert" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <h3 id="next-steps">Next Steps</h3> <p>I have prepared a mid-term summary video, which will be submitted for feedback. This video summarizes the current status of the project. Based on the feedback received, we will make the necessary adjustments to ensure that the project continues to meet its objectives and align with the broader goals of the GSoC initiative.</p> <p>Moving forward, we will delve deeper into the literature, exploring new research directions that could lead to improvements in the model’s performance and its integration within the CARLA simulation environment.</p>]]></content><author><name>Zebin Huang</name></author><category term="gsoc-2024"/><category term="Weekly_blogs"/><summary type="html"><![CDATA[Over the past two weeks, our focus has been on addressing several tasks for the ongoing development of our project. Here is a summary of the key action items, progress, and additional insights that have emerged during this period:]]></summary></entry></feed>